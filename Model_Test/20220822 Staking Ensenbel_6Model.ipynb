{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15971,"status":"ok","timestamp":1661209966220,"user":{"displayName":"Jaewoo Choi","userId":"05968598841946004036"},"user_tz":-540},"id":"h6h89BPK1CaA","outputId":"4022c5d9-2fa8-4b1e-9545-7410a88b7913"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35443,"status":"ok","timestamp":1661210001660,"user":{"displayName":"Jaewoo Choi","userId":"05968598841946004036"},"user_tz":-540},"id":"V1s0gwOEkAYi","outputId":"f807e119-a943-48cd-dd33-657e38379608"},"outputs":[{"name":"stdout","output_type":"stream","text":["Archive:  /content/drive/MyDrive/Project/Dacon/lgauto/open.zip\n","   creating: meta/\n","  inflating: meta/x_feature_info.csv  \n","  inflating: meta/y_feature_info.csv  \n","  inflating: meta/y_feature_spec_info.csv  \n","  inflating: sample_submission.csv   \n","  inflating: test.csv                \n","  inflating: train.csv               \n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting optuna\n","  Downloading optuna-2.10.1-py3-none-any.whl (308 kB)\n","\u001b[K     |████████████████████████████████| 308 kB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.40)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.21.6)\n","Collecting cliff\n","  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n","\u001b[K     |████████████████████████████████| 81 kB 10.4 MB/s \n","\u001b[?25hRequirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.7.3)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n","Collecting colorlog\n","  Downloading colorlog-6.6.0-py2.py3-none-any.whl (11 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.64.0)\n","Collecting alembic\n","  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\n","\u001b[K     |████████████████████████████████| 209 kB 72.7 MB/s \n","\u001b[?25hCollecting cmaes>=0.8.2\n","  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.9)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (4.12.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (1.1.2)\n","Collecting Mako\n","  Downloading Mako-1.2.1-py3-none-any.whl (78 kB)\n","\u001b[K     |████████████████████████████████| 78 kB 7.6 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (5.9.0)\n","Collecting autopage>=0.4.0\n","  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n","Collecting pbr!=2.1.0,>=2.0.0\n","  Downloading pbr-5.10.0-py2.py3-none-any.whl (112 kB)\n","\u001b[K     |████████████████████████████████| 112 kB 90.1 MB/s \n","\u001b[?25hCollecting cmd2>=1.0.0\n","  Downloading cmd2-2.4.2-py3-none-any.whl (147 kB)\n","\u001b[K     |████████████████████████████████| 147 kB 84.2 MB/s \n","\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.3.0)\n","Collecting stevedore>=2.0.1\n","  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n","\u001b[K     |████████████████████████████████| 49 kB 7.1 MB/s \n","\u001b[?25hRequirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (22.1.0)\n","Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n","Collecting pyperclip>=1.6\n","  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (4.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.8.1)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna) (2.0.1)\n","Building wheels for collected packages: pyperclip\n","  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=bed6673c988b0228c1bc0f6eecb542e6b4e4e16f35e0df2fead5790da8f3b63f\n","  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n","Successfully built pyperclip\n","Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n","Successfully installed Mako-1.2.1 alembic-1.8.1 autopage-0.5.1 cliff-3.10.1 cmaes-0.8.2 cmd2-2.4.2 colorlog-6.6.0 optuna-2.10.1 pbr-5.10.0 pyperclip-1.8.2 stevedore-3.5.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting catboost\n","  Downloading catboost-1.0.6-cp37-none-manylinux1_x86_64.whl (76.6 MB)\n","\u001b[K     |████████████████████████████████| 76.6 MB 149 kB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0)\n","Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.3.5)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (5.5.0)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.7.3)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.21.6)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2022.2.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (3.0.9)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (1.4.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->catboost) (4.1.1)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost) (8.0.1)\n","Installing collected packages: catboost\n","Successfully installed catboost-1.0.6\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting skranger\n","  Downloading skranger-0.7.0-cp37-cp37m-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (405 kB)\n","\u001b[K     |████████████████████████████████| 405 kB 5.1 MB/s \n","\u001b[?25hCollecting scikit-learn<1,>=0.23.0\n","  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n","\u001b[K     |████████████████████████████████| 22.3 MB 90.2 MB/s \n","\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<1,>=0.23.0->skranger) (3.1.0)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<1,>=0.23.0->skranger) (1.21.6)\n","Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<1,>=0.23.0->skranger) (1.7.3)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<1,>=0.23.0->skranger) (1.1.0)\n","Installing collected packages: scikit-learn, skranger\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.0.2\n","    Uninstalling scikit-learn-1.0.2:\n","      Successfully uninstalled scikit-learn-1.0.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.24.2 which is incompatible.\u001b[0m\n","Successfully installed scikit-learn-0.24.2 skranger-0.7.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ngboost\n","  Downloading ngboost-0.3.12-py3-none-any.whl (31 kB)\n","Requirement already satisfied: tqdm>=4.3 in /usr/local/lib/python3.7/dist-packages (from ngboost) (4.64.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from ngboost) (1.21.6)\n","Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.7/dist-packages (from ngboost) (0.24.2)\n","Requirement already satisfied: scipy>=1.3 in /usr/local/lib/python3.7/dist-packages (from ngboost) (1.7.3)\n","Collecting lifelines>=0.25\n","  Downloading lifelines-0.27.1-py3-none-any.whl (349 kB)\n","\u001b[K     |████████████████████████████████| 349 kB 7.9 MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.7/dist-packages (from lifelines>=0.25->ngboost) (3.2.2)\n","Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from lifelines>=0.25->ngboost) (1.3.5)\n","Collecting autograd-gamma>=0.3\n","  Downloading autograd-gamma-0.5.0.tar.gz (4.0 kB)\n","Collecting formulaic>=0.2.2\n","  Downloading formulaic-0.4.0-py3-none-any.whl (76 kB)\n","\u001b[K     |████████████████████████████████| 76 kB 6.1 MB/s \n","\u001b[?25hRequirement already satisfied: autograd>=1.3 in /usr/local/lib/python3.7/dist-packages (from lifelines>=0.25->ngboost) (1.4)\n","Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.7/dist-packages (from autograd>=1.3->lifelines>=0.25->ngboost) (0.16.0)\n","Requirement already satisfied: cached_property>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from formulaic>=0.2.2->lifelines>=0.25->ngboost) (1.5.2)\n","Requirement already satisfied: astor>=0.8 in /usr/local/lib/python3.7/dist-packages (from formulaic>=0.2.2->lifelines>=0.25->ngboost) (0.8.1)\n","Collecting graphlib-backport<2.0.0,>=1.0.0\n","  Downloading graphlib_backport-1.0.3-py3-none-any.whl (5.1 kB)\n","Collecting typing-extensions<5.0.0,>=4.2.0\n","  Downloading typing_extensions-4.3.0-py3-none-any.whl (25 kB)\n","Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.7/dist-packages (from formulaic>=0.2.2->lifelines>=0.25->ngboost) (1.14.1)\n","Collecting interface-meta<2.0.0,>=1.2.0\n","  Downloading interface_meta-1.3.0-py3-none-any.whl (14 kB)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (1.4.4)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (3.0.9)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.0->lifelines>=0.25->ngboost) (2022.2.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.0->lifelines>=0.25->ngboost) (1.15.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21->ngboost) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21->ngboost) (1.1.0)\n","Building wheels for collected packages: autograd-gamma\n","  Building wheel for autograd-gamma (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4048 sha256=9103fbe1fbdd6d32390bff9126a57a61ab3de8cc74076f62a8898573fc5a1eee\n","  Stored in directory: /root/.cache/pip/wheels/9f/01/ee/1331593abb5725ff7d8c1333aee93a50a1c29d6ddda9665c9f\n","Successfully built autograd-gamma\n","Installing collected packages: typing-extensions, interface-meta, graphlib-backport, formulaic, autograd-gamma, lifelines, ngboost\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing-extensions 4.1.1\n","    Uninstalling typing-extensions-4.1.1:\n","      Successfully uninstalled typing-extensions-4.1.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","thinc 8.1.0 requires typing-extensions<4.2.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.3.0 which is incompatible.\n","spacy 3.4.1 requires typing-extensions<4.2.0,>=3.7.4; python_version < \"3.8\", but you have typing-extensions 4.3.0 which is incompatible.\u001b[0m\n","Successfully installed autograd-gamma-0.5.0 formulaic-0.4.0 graphlib-backport-1.0.3 interface-meta-1.3.0 lifelines-0.27.1 ngboost-0.3.12 typing-extensions-4.3.0\n"]}],"source":["!unzip /content/drive/MyDrive/Project/Dacon/lgauto/open.zip\n","# !git clone --recursive https://github.com/Microsoft/LightGBM\n","# !cd LightGBM && rm -rf build && mkdir build && cd build && cmake -DUSE_GPU=1 ../../LightGBM && make -j4 && cd ../python-package && python3 setup.py install --precompile --gpu;\n","!pip install optuna\n","!pip install catboost\n","!pip install skranger\n","!pip install ngboost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pcG2aSVkkAVz"},"outputs":[],"source":["import pandas as pd\n","import random\n","import os\n","import warnings\n","warnings.filterwarnings('ignore')\n","import numpy as np\n","import tqdm\n","\n","from sklearn.model_selection import cross_val_score\n","from sklearn import metrics\n","from sklearn.model_selection import train_test_split, KFold\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import make_scorer\n","from sklearn.multioutput import MultiOutputRegressor\n","from sklearn.model_selection import KFold\n","\n","from lightgbm import LGBMRegressor\n","from ngboost import NGBRegressor\n","from sklearn.svm import SVR\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","from xgboost import XGBRegressor\n","from sklearn.linear_model import ElasticNet, LinearRegression, Lasso, Ridge\n","from catboost import CatBoostRegressor, Pool\n","from skranger.ensemble import RangerForestRegressor\n","from sklearn.neighbors import RadiusNeighborsRegressor\n","\n","from hyperopt import fmin, hp, tpe\n","from sklearn import metrics\n","from sklearn.model_selection import cross_val_score\n","from sklearn.metrics import make_scorer\n","from sklearn.multioutput import MultiOutputRegressor\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import mean_squared_error\n","from sklearn.inspection import permutation_importance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8e67ifiDnv5w"},"outputs":[],"source":["class Config:\n","  seed = 42\n","  epochs = 200"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ZK7vgIBkAS-"},"outputs":[],"source":["def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V8XIRg1wkAQW"},"outputs":[],"source":["def dataset_split_X_y(df):    \n","    \"\"\"\n","    @Description: split data into features and labels\n","    @Param: df, pandas dataframe with columns starting with X for features and Y for labels\n","    @Return: features and labels in pandas dataframes\n","    \"\"\"\n","    xs = df.filter(regex='X') # Input : X Feature\n","    ys = df.filter(regex='Y') # Output : Y Feature\n","    return xs, ys"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KBgkm1KPlYtZ"},"outputs":[],"source":["def check_for_NAs(df, show=False):\n","    \"\"\"\n","    @Description: checks for the NAs in the dataframe\n","    @Param1: df, pandas dataframe\n","    @Param2: show, boolean indicating whether NaN data are also necessary as a part of the output\n","    @Return: name of the columns with NaN\n","    \"\"\"\n","    nan_values = df.loc[:, df.isnull().any()]\n","    if show:\n","        return df[df.isna().any(axis=1)]\n","    return list(nan_values.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZgMxozATlbCp"},"outputs":[],"source":["def check_for_label_bound(df, labels, bound):\n","    \"\"\"\n","    @Description: check bound is inbetween min and max\n","    @Param1: df, pandas dataframe\n","    @Param2: labels, list of column names \n","    @Param3: thres: list of bounds\n","    @Return: names of the columns not within the bound\n","    \"\"\"\n","    n = len(labels)\n","    result = []\n","    for idx in range(n):\n","        col = labels[idx]\n","        thres = bound[idx]\n","        extracted_column = df[col]\n","        if not extracted_column.between(thres[0], thres[1]).all():\n","            result.append(labels[idx])\n","    if len(result) == 0:\n","        print('everything is within the bound')\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fA_w1Rt-kADH"},"outputs":[],"source":["def zero_variance(df):\n","    \"\"\"\n","    @Description: check for zero_variance\n","    @Param1: df, pandas dataframe\n","    @Return: names of the columns with zero variance\n","    \"\"\"\n","    result = []\n","    for col in df.columns:\n","        if df[col].var() == 0:\n","            result.append(col)\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NtEGKh0zlErb"},"outputs":[],"source":["def get_top_correlation(df, n=10):\n","    \"\"\"\n","    @Description: print out top correlated features\n","    @Param1: df, pandas dataframe\n","    @Param2: n, number of lines to print \n","    @Return: pandas series\n","    \"\"\"\n","    pairs = set()\n","    for idx1 in range(0, df.shape[1]):\n","        for idx2 in range(0, idx1+1):\n","            pairs.add((df.columns[idx1], df.columns[idx2]))\n","    corr = df.corr().abs().unstack()\n","    corr = corr.drop(labels=pairs).sort_values(ascending=False)\n","    return corr[0:n]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FlUrH-kLldrb"},"outputs":[],"source":["def adjacent_histogram_boxplot(feature_var, figsize = (7, 5)):\n","    \"\"\"\n","    @Description: plot histogram and boxplot in next to each other\n","    @Param1: feature_var, pandas series \n","    @Param2: figsize, size of the figure \n","    \"\"\"\n","    fig, (hist_plot, box_plot) = plt.subplots(nrows=2, sharex=True, gridspec_kw={'height_ratios':(.85,.15)}, figsize=figsize)\n","    sns.distplot(feature_var, kde=True, ax=hist_plot, kde_kws= {\"linewidth\":1.5}) \n","    sns.boxplot(feature_var, ax=box_plot, linewidth = 1, width = 0.5)\n","    hist_plot.set_ylabel('')    \n","    hist_plot.set_xlabel('')\n","    box_plot.set_xlabel('')\n","    hist_plot.tick_params(labelsize=8)\n","    box_plot.tick_params(labelsize=8)\n","    fig.suptitle(feature_var.name, fontsize = 10)\n","    hist_plot.axvline(np.mean(feature_var),color='red',linestyle='-',lw = 1.5)\n","    hist_plot.axvline(np.median(feature_var),color='green',linestyle='--',lw = 1.5)\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6CEAj3XQlEgR"},"outputs":[],"source":["def lg_nrmse(gt, preds):\n","    \"\"\"\n","    @Description: Metric used in this project\n","    @Params1: gt, pandas dataframe\n","    @Param2: preds, pandas dataframe\n","    @Return: nrmse score\n","    \"\"\"\n","    # 각 Y Feature별 NRMSE 총합\n","    # Y_01 ~ Y_08 까지 20% 가중치 부여\n","    preds = pd.DataFrame(preds)\n","    all_nrmse = []\n","    for idx in range(0,14):\n","        rmse = mean_squared_error(gt.iloc[:,idx], preds.iloc[:,idx], squared=False)\n","        nrmse = rmse/np.mean(np.abs(gt.iloc[:,idx]))\n","        all_nrmse.append(nrmse)\n","    score = 1.2 * np.sum(all_nrmse[:8]) + 1.0 * np.sum(all_nrmse[8:15])\n","    return score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HfB2waRKlEdS"},"outputs":[],"source":["def lg_individual_nrmse(gt, preds):\n","    \"\"\"\n","    @Description: Metric used in this project (individual)\n","    @Params1: gt, pandas dataframe\n","    @Param2: preds, pandas dataframe\n","    @Return: nrmse score\n","    \"\"\"\n","    # 각 Y Feature별 NRMSE 총합\n","    # Y_01 ~ Y_08 까지 20% 가중치 부여\n","    rmse = mean_squared_error(gt, preds, squared=False)\n","    nrmse = rmse/np.mean(np.abs(gt))\n","    return nrmse"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-zIBOcBUliiY"},"outputs":[],"source":["def find_outlier_zscore(data, threshold = 3):\n","    mean = np.mean(data)\n","    std = np.std(data)\n","    zs = [(y - mean) / std for y in data]\n","    masks = np.where(np.abs(zs) > threshold)\n","    return masks[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QenNjbcYlkM5"},"outputs":[],"source":["ys = ['Y_01', 'Y_02', 'Y_03', 'Y_04', 'Y_05', \n","      'Y_06', 'Y_07', 'Y_08', 'Y_09', 'Y_10', \n","      'Y_11', 'Y_12', 'Y_13', 'Y_14']\n","ys_bounds = [[0.2, 2], [0.2, 2.1], [0.2, 2.1], \n","             [7, 19], [22, 36.5], [-19.2, 19], \n","             [2.4, 4], [-29.2, -24], [-29.2, -24],\n","             [-30.6, -20], [19.6, 26.6], [-29.2, -24],\n","             [-29.2, -24], [-29.2, -24]]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FB59rtt-llsC"},"outputs":[],"source":["seed_everything(Config.seed)\n","\n","train_df = pd.read_csv('./train.csv')\n","test_x = pd.read_csv('./test.csv')\n","train_x, train_y = dataset_split_X_y(train_df)\n","\n","cols_with_zero_variance = zero_variance(train_x) # 분산이 0 (통과 여부)\n","train_x = train_x.drop(cols_with_zero_variance, axis = 1)\n","test_x = test_x.drop(cols_with_zero_variance, axis = 1)\n","\n","train_x = train_x.drop(['X_10', 'X_11'], axis = 1) # 결측치가 많음 (결측치 = 0, 공지사항)\n","test_x = test_x.drop(['X_10', 'X_11'], axis = 1)\n","\n","test_x = test_x.drop('ID', axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uWxyzIilqLVx"},"outputs":[],"source":["class Config:\n","  seed = 42\n","  epochs = 200\n","  cv=10\n","  test_size = 0.2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PhFxUQZhklLd"},"outputs":[],"source":["def lgbm_objective(params):\n","    params = {\n","        'n_estimators': int(params['n_estimators']),\n","        'max_depth': int(params['max_depth']),\n","        'num_leaves': int(params['num_leaves']),\n","        'min_child_samples': int(params['min_child_samples']),\n","        'colsample_bytree': '{:.5f}'.format(params['colsample_bytree']),\n","        'subsample': '{:.5f}'.format(params['subsample']),\n","        'min_split_gain': '{:.5f}'.format(params['min_split_gain']),\n","        'scale_pos_weight': '{:.5f}'.format(params['scale_pos_weight']),\n","        'reg_alpha': '{:.5f}'.format(params['reg_alpha']),\n","        'reg_lambda': '{:.5f}'.format(params['reg_lambda']),\n","        'learning_rate': '{:.5f}'.format(params['learning_rate']),   \n","    }\n","\n","    model = LGBMRegressor(\n","        n_jobs = -1,\n","        random_state = 1,\n","        verbose = 100,\n","        **params\n","    )\n","\n","    losses = np.sqrt(-cross_val_score(model, train_x, train_y['Y_01'], cv=Config.cv, scoring='neg_mean_squared_error'))\n","    losses = losses / np.mean(np.abs(train_y['Y_01']))\n","    return losses.mean()\n","\n","\n","def xgb_objective(params):\n","    params = {\n","\n","    }\n","\n","    model = XGBRegressor(\n","        n_jobs = -1,\n","        verbose = 100,\n","        random_state = 1,\n","        **params\n","    )\n","\n","    losses = np.sqrt(-cross_val_score(model, train_x, train_y['Y_01'], cv=Config.cv, scoring='neg_mean_squared_error'))\n","    losses = losses / np.mean(np.abs(train_y['Y_01']))\n","    return losses.mean()\n","\n","\n","def cat_objective(params):\n","    params = {\n","        'n_estimators': int(params['n_estimators']),\n","        'depth': int(params['depth']),\n","        'learning_rate': params['learning_rate'],   \n","        'l2_leaf_reg': params['l2_leaf_reg'],\n","        'max_bin': int(params['max_bin']),\n","        'min_data_in_leaf': int(params['min_data_in_leaf']),\n","        'random_strength': params['random_strength'],\n","        'fold_len_multiplier': params['fold_len_multiplier'],\n","        \n","    }\n","\n","    model = CatBoostRegressor(\n","        logging_level='Silent',\n","        **params\n","    )\n","\n","    losses = np.sqrt(-cross_val_score(model, train_x, train_y['Y_04'], cv=Config.cv, scoring='neg_mean_squared_error'))\n","    losses = losses / np.mean(np.abs(train_y['Y_04']))\n","    return losses.mean()\n","\n","def random_objective(params):\n","    params = {\n"," \n","    }\n","\n","    model = RangerForestRegressor(\n","        n_jobs = -1,\n","        verbose= 100\n","        **params\n","    )\n","\n","    losses = np.sqrt(-cross_val_score(model, train_x, train_y['Y_01'], cv=Config.cv, scoring='neg_mean_squared_error'))\n","    losses = losses / np.mean(np.abs(train_y['Y_01']))\n","    return losses.mean()\n","\n","def gradient_objective(params):\n","    params = {\n","\n","        'n_estimators': int(params['n_estimators']),\n","        'max_depth': int(params['max_depth']),\n","        'subsample': params['subsample'],\n","        'learning_rate': params['learning_rate'],\n","        'min_samples_split': int(params['min_samples_split']),\n","        'min_samples_leaf': int(params['min_samples_leaf']),\n","        'min_weight_fraction_leaf': params['min_weight_fraction_leaf'],\n","        'min_impurity_decrease': params['min_impurity_decrease'],\n","        'max_features': params['max_features'],\n","        'alpha': params['alpha'],\n","        'max_leaf_nodes': int(params['max_leaf_nodes']),\n","        'ccp_alpha': params['ccp_alpha'],\n","        \n","    }\n","\n","    model = GradientBoostingRegressor(\n","        random_state = 1,\n","        **params\n","    )\n","    \n","    losses = np.sqrt(-cross_val_score(model, train_x, train_y['Y_07'], cv=Config.cv, scoring='neg_mean_squared_error'))\n","    losses = losses / np.mean(np.abs(train_y['Y_07']))\n","    print(\"NRMSE Loss {:.5f} params {}\".format(losses.mean(), params))\n","    \n","    return losses.mean()\n","\n","def extra_objective(params):\n","    params = {\n","        'n_estimators': int(params['n_estimators']),\n","        'max_depth': int(params['max_depth']),\n","        'min_samples_split': int(params['min_samples_split']),\n","        'min_samples_leaf': int(params['min_samples_leaf']),\n","        'min_weight_fraction_leaf': params['min_weight_fraction_leaf'],\n","        'max_features': params['max_features'],\n","        'max_leaf_nodes': int(params['max_leaf_nodes']),\n","        'min_impurity_decrease': params['min_impurity_decrease'],\n","        'bootstrap': params['bootstrap'],\n","        'ccp_alpha': params['ccp_alpha'],  \n","    }\n","\n","    model = ExtraTreesRegressor(\n","        n_jobs = -1,\n","        verbose = 0,\n","        random_state = 1,\n","        **params\n","    )\n","\n","    losses = np.sqrt(-cross_val_score(model, train_x, train_y['Y_01'], cv=Config.cv, scoring='neg_mean_squared_error'))\n","    losses = losses / np.mean(np.abs(train_y['Y_01']))\n","    return losses.mean()\n","\n","def ngbr_objective(params):\n","    params = {\n","        'n_estimators': int(params['n_estimators']),\n","        'learning_rate': params['learning_rate'],\n","        'natural_gradient': params['natural_gradient'],\n","        'col_sample': float(params['col_sample']),\n","        'minibatch_frac': float(params['minibatch_frac']),\n","        'tol': float(params['tol']),\n","    }\n","\n","    model = NGBRegressor(\n","        verbose = 100,\n","        random_state = 1,\n","        **params\n","    )\n","\n","    losses = np.sqrt(-cross_val_score(model, train_x, train_y['Y_01'], cv=Config.cv, scoring='neg_mean_squared_error'))\n","    losses = losses / np.mean(np.abs(train_y['Y_01']))\n","    print(\"NRMSE Loss {:.5f} params {}\".format(losses.mean(), params))\n","    return losses.mean()"]},{"cell_type":"markdown","metadata":{"id":"AJEAijvRaiPb"},"source":["## Catboost Regressor"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"qlT0GKmqaj_H","outputId":"f62ed8e1-48f5-4977-d773-c5d745a4f047"},"outputs":[{"name":"stdout","output_type":"stream","text":["100%|██████████| 200/200 [35:40<00:00, 10.70s/it, best loss: 0.1916584045864259]\n"]}],"source":["## https://catboost.ai/en/docs/concepts/parameter-tuning (참고)\n","space_catboost = {\n","    'n_estimators' : hp.quniform('n_estimators', 10, 20, 1),\n","    'depth': hp.quniform(\"depth\", 2, 16, 1),\n","    'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n","    'l2_leaf_reg': hp.uniform('l2_leaf_reg', 3, 8),\n","    'max_bin' : hp.quniform('max_bin', 1, 254, 1),\n","    'min_data_in_leaf' : hp.quniform('min_data_in_leaf', 2, 700, 1),\n","    'random_strength' : hp.loguniform('random_strength', np.log(0.005), np.log(5)),\n","    'fold_len_multiplier' : hp.loguniform('fold_len_multiplier', np.log(1.01), np.log(2.5)),\n","}\n","\n","best = fmin(fn = cat_objective,\n","            space = space_catboost,\n","            algo = tpe.suggest,\n","            verbose = 1,\n","            max_evals = 200)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"33Rrmodh1OpG"},"outputs":[],"source":["print(best)"]},{"cell_type":"markdown","metadata":{"id":"Z42ZYLiqFDSV"},"source":["## Extra Trees Regressor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CfgLLU2OFKg0"},"outputs":[],"source":["space_extra = {\n","    'n_estimators' : hp.quniform('n_estimators', 100, 1500, 50),\n","    'max_depth': hp.quniform('max_depth', 3, 50, 1),\n","    'min_samples_split': hp.quniform('min_samples_split', 5, 50, 5),\n","    'min_samples_leaf': hp.quniform('min_samples_leaf', 5, 50, 1),\n","    'min_weight_fraction_leaf': hp.uniform('min_weight_fraction_leaf', 0.01, 0.5),\n","    'max_features': hp.choice('max_features', ['sqrt', 'log2', None, 'auto']),\n","    'max_leaf_nodes': hp.quniform('max_leaf_nodes', 3, 30, 1),\n","    'min_impurity_decrease': hp.uniform('min_impurity_decrease', 0, 200),\n","    'bootstrap':  hp.choice('bootstrap', [True, False]),\n","    'ccp_alpha': hp.uniform('ccp_alpha', 0.01, 1.0),\n","}\n","\n","best = fmin(fn = extra_objective,\n","            space = space_extra,\n","            algo = tpe.suggest,\n","            verbose = 1,\n","            max_evals = 2)\n","\n","best['n_estimators'] = int(best['n_estimators'])\n","best['max_depth'] = int(best['max_depth'])\n","best['max_leaf_nodes'] = int(best['max_leaf_nodes'])\n"]},{"cell_type":"markdown","metadata":{"id":"lnfWPjbxVTvl"},"source":["# RangerForest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1UMa6CGRVWWu"},"outputs":[],"source":["def random_objective(params):\n","    params = {\n","        'n_estimators': int(params['n_estimators']),\n","        'mtry': int(params['mtry']),\n","        'min_node_size': int(params['min_node_size']),\n","        'max_depth': int(params['max_depth']),\n","        # 'num_random_splits': int(params['num_random_splits']),\n","        'sample_fraction': params['sample_fraction'],\n","        'alpha':  params['alpha'],\n","        # 'split_rule' : params['split_rule'],\n","    }\n","\n","    model = RangerForestRegressor(\n","        n_jobs = -1,\n","        **params\n","    )\n","\n","    losses = np.sqrt(-cross_val_score(model, train_x, train_y['Y_04'], cv=Config.cv, scoring='neg_mean_squared_error'))\n","    losses = losses / np.mean(np.abs(train_y['Y_04']))\n","    return losses.mean()\n","\n","space_random = {\n","    'n_estimators' : hp.quniform('n_estimators', 100, 1500, 1),\n","    'mtry': hp.quniform('mtry', 5, len(train_x.columns), 1),\n","    'min_node_size': hp.quniform('min_node_size', 10, 200, 5),\n","    'max_depth': hp.quniform('max_depth', 10, 350, 5),\n","    # 'num_random_splits': hp.quniform('num_random_splits', 5, 200, 5),\n","    'sample_fraction': hp.uniform('sample_fraction', 0.3, 1.0),\n","    'alpha': hp.uniform('alpha', 0.3, 1.0),\n","    # 'split_rule' : hp.choice('reg_lambda', ['variance', 'extratrees', 'maxstat', 'beta']),\n","}\n","\n","best = fmin(fn = random_objective,\n","            space = space_random,\n","            algo = tpe.suggest,\n","            max_evals = 200)\n","\n","print(best)"]},{"cell_type":"markdown","metadata":{"id":"lzh414tCeS6k"},"source":["## Gradient Boost Objective\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S6jCCmj1ePRO"},"outputs":[],"source":["space_gradient = {\n","    'n_estimators' : hp.quniform('n_estimators', 100, 2000, 10),\n","    'max_depth': hp.quniform('max_depth', 5, 250, 1),\n","    'subsample': hp.uniform('subsample', 0.3, 1.0),\n","    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.5)),\n","    'min_samples_split': hp.quniform('min_samples_split', 5, 50, 5),\n","    'min_samples_leaf': hp.quniform('min_samples_leaf', 5, 50, 1),\n","    'min_weight_fraction_leaf': hp.uniform('min_weight_fraction_leaf', 0.01, 0.5),\n","    'min_impurity_decrease': hp.uniform('min_impurity_decrease', 0, 200),\n","    'max_features': hp.choice('max_features', ['sqrt', 'log2', None, 'auto']),\n","    'alpha': hp.uniform('alpha', 0.001, 0.999),\n","    'max_leaf_nodes': hp.quniform('max_leaf_nodes', 3, 30, 1),\n","    'ccp_alpha': hp.uniform('ccp_alpha', 0.01, 0.999),\n","\n","}\n","\n","best = fmin(fn = gradient_objective,\n","            space = space_gradient,\n","            algo = tpe.suggest,\n","            verbose = 1,\n","            max_evals = 200)\n","\n","print(best)"]},{"cell_type":"markdown","metadata":{"id":"f6_7kTkbVZJh"},"source":["##LGBM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H9E4wigN9rTR"},"outputs":[],"source":["space_lgbm = {\n","    'n_estimators' : hp.quniform('n_estimators', 100, 1500, 1),\n","    'max_depth': hp.quniform('max_depth', 5, 250, 1),\n","    'num_leaves': hp.quniform('num_leaves', 20, 200, 5),\n","    'min_child_samples': hp.quniform('min_child_samples', 10, 150, 5),\n","    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1.0),\n","    'subsample': hp.uniform('subsample', 0.3, 1.0),\n","    'min_split_gain': hp.uniform('min_split_gain', 0, 0.7),\n","    'scale_pos_weight': hp.uniform('scale_pos_weight', 1, 10),\n","    'reg_alpha': hp.uniform('reg_alpha', 0, 500),\n","    'reg_lambda': hp.uniform('reg_lambda', 0, 500),\n","    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.5)),\n","}\n","\n","best = fmin(fn = lgbm_objective,\n","            space = space_lgbm,\n","            algo = tpe.suggest,\n","            verbose = 10,\n","            max_evals = 200)\n","\n","print(best)\n","best['n_estimators'] = int(best['n_estimators'])\n","best['num_leaves'] = int(best['num_leaves'])\n","best['max_depth'] = int(best['max_depth'])\n","best['min_child_samples'] = int(best['min_child_samples'])"]},{"cell_type":"markdown","source":["##NGBR"],"metadata":{"id":"ZoMrNNFK8u00"}},{"cell_type":"code","source":["space_ngboost = {\n","    'n_estimators': hp.quniform('n_estimators', 100, 2000, 10),\n","    'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n","    'natural_gradient': hp.choice('natural_gradient', [True, False]),\n","    'col_sample': hp.quniform('col_sample', 0, 1, 0.01),\n","    'minibatch_frac': hp.quniform('minibatch_frac', 0, 1, 0.01),\n","    'tol': hp.uniform('tol', 1e-6, 3e-4),\n","}\n","\n","best = fmin(fn = ngbr_objective,\n","            space = space_ngboost,\n","            algo = tpe.suggest,\n","            verbose = 10,\n","            max_evals = 100)\n","\n","print(best)\n","best['n_estimators'] = int(best['n_estimators'])\n","best['num_leaves'] = int(best['num_leaves'])\n","best['max_depth'] = int(best['max_depth'])\n","best['min_child_samples'] = int(best['min_child_samples'])"],"metadata":{"id":"97ITzGb_8ydY"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qlA8SXYc9_5t"},"outputs":[],"source":["def get_stacking_base_datasets(model, train_x, train_y, col,test):\n","    kf = KFold(n_splits=Config.cv, shuffle=False)\n","    train_fold_pred = np.zeros((train_x.shape[0],1))\n","    test_pred = np.zeros((test.shape[0],Config.cv))\n","    \n","    \n","    for folder_counter, (train_index, valid_index) in enumerate(kf.split(train_x)):\n","        print('Fold : ', folder_counter, ' Start')\n","        X_tr = train_x.loc[train_index]\n","        y_tr = train_y[col].loc[train_index]\n","        X_te = train_x.loc[valid_index] \n","        \n","        model.fit(X_tr, y_tr)\n","        train_fold_pred[valid_index, :] = model.predict(X_te).reshape(-1,1) \n","        test_pred[:, folder_counter] = model.predict(test) \n","        \n","    test_pred_mean = np.mean(test_pred, axis=1).reshape(-1,1)\n","    \n","    return train_fold_pred, test_pred_mean "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sOps13z9-StG"},"outputs":[],"source":["model = LGBMRegressor(\n","        n_jobs = -1,\n","        random_state = 1,\n","        verbose = 100,\n","        **best\n","    )\n","\n","\n","# model 8개\n","xx_train, xx_test = get_stacking_base_datasets(model, train_x, train_y, col='Y_01', test=test_x)\n","yy_train, yy_test = get_stacking_base_datasets(model, train_x, train_y, col='Y_01', test=test_x)\n","zz_train, zz_test = get_stacking_base_datasets(model, train_x, train_y, col='Y_01', test=test_x)\n","qq_train, qq_test = get_stacking_base_datasets(model, train_x, train_y, col='Y_01', test=test_x)\n","\n","Stack_final_X_train = np.concatenate((xx_train,yy_train,zz_train,qq_train), axis=1)\n","Stack_final_X_test = np.concatenate((xx_test,yy_test,zz_test,qq_test), axis=1)\n","\n","# final_model 선택해야함\n","\bfinal_model.fit(Stack_final_X_train, y_train)\n","stack_final = \bfinal_model.predict(Stack_final_X_test) \n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VSh84dg_WnrV"},"outputs":[],"source":["## col1 col2 지정\n","stack_final.to_csv(f'{col1}_{col2}.csv', index=False)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"FINAL.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}