{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h6h89BPK1CaA",
    "outputId": "a8843b6a-2099-4d0f-a975-dfaec0a0dd94"
   },
   "source": [
    "# Stacking Ensenble을 활용한 자율주행 센서 안테나 성능예측\n",
    "> 팀명 : 될때까지간다리\n",
    ">\n",
    "> 작성일 : '22.08.31\n",
    ">\n",
    "> 개발환경 : Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development Environment Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V1s0gwOEkAYi",
    "outputId": "929c6c3e-2a40-49e6-8944-331578af424a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in c:\\dev\\miniconda3\\lib\\site-packages (2.10.1)\n",
      "Requirement already satisfied: tqdm in c:\\dev\\miniconda3\\lib\\site-packages (from optuna) (4.63.0)\n",
      "Requirement already satisfied: alembic in c:\\dev\\miniconda3\\lib\\site-packages (from optuna) (1.8.1)\n",
      "Requirement already satisfied: colorlog in c:\\dev\\miniconda3\\lib\\site-packages (from optuna) (6.6.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.1.0 in c:\\dev\\miniconda3\\lib\\site-packages (from optuna) (1.4.40)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\dev\\miniconda3\\lib\\site-packages (from optuna) (21.3)\n",
      "Requirement already satisfied: PyYAML in c:\\dev\\miniconda3\\lib\\site-packages (from optuna) (6.0)\n",
      "Requirement already satisfied: scipy!=1.4.0 in c:\\dev\\miniconda3\\lib\\site-packages (from optuna) (1.9.0)\n",
      "Requirement already satisfied: cliff in c:\\dev\\miniconda3\\lib\\site-packages (from optuna) (4.0.0)\n",
      "Requirement already satisfied: numpy in c:\\dev\\miniconda3\\lib\\site-packages (from optuna) (1.23.2)\n",
      "Requirement already satisfied: cmaes>=0.8.2 in c:\\dev\\miniconda3\\lib\\site-packages (from optuna) (0.8.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\dev\\miniconda3\\lib\\site-packages (from packaging>=20.0->optuna) (3.0.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\dev\\miniconda3\\lib\\site-packages (from sqlalchemy>=1.1.0->optuna) (1.1.2)\n",
      "Requirement already satisfied: Mako in c:\\dev\\miniconda3\\lib\\site-packages (from alembic->optuna) (1.2.1)\n",
      "Requirement already satisfied: cmd2>=1.0.0 in c:\\dev\\miniconda3\\lib\\site-packages (from cliff->optuna) (2.4.2)\n",
      "Requirement already satisfied: stevedore>=2.0.1 in c:\\dev\\miniconda3\\lib\\site-packages (from cliff->optuna) (4.0.0)\n",
      "Requirement already satisfied: PrettyTable>=0.7.2 in c:\\dev\\miniconda3\\lib\\site-packages (from cliff->optuna) (3.3.0)\n",
      "Requirement already satisfied: autopage>=0.4.0 in c:\\dev\\miniconda3\\lib\\site-packages (from cliff->optuna) (0.5.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\dev\\miniconda3\\lib\\site-packages (from cliff->optuna) (4.12.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\dev\\miniconda3\\lib\\site-packages (from cmd2>=1.0.0->cliff->optuna) (3.4.1)\n",
      "Requirement already satisfied: pyperclip>=1.6 in c:\\dev\\miniconda3\\lib\\site-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in c:\\dev\\miniconda3\\lib\\site-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
      "Requirement already satisfied: attrs>=16.3.0 in c:\\dev\\miniconda3\\lib\\site-packages (from cmd2>=1.0.0->cliff->optuna) (21.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\dev\\miniconda3\\lib\\site-packages (from importlib-metadata>=4.4->cliff->optuna) (3.8.1)\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in c:\\dev\\miniconda3\\lib\\site-packages (from stevedore>=2.0.1->cliff->optuna) (5.10.0)\n",
      "Requirement already satisfied: colorama in c:\\dev\\miniconda3\\lib\\site-packages (from colorlog->optuna) (0.4.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\dev\\miniconda3\\lib\\site-packages (from Mako->alembic->optuna) (2.1.1)\n",
      "Requirement already satisfied: catboost in c:\\dev\\miniconda3\\lib\\site-packages (1.0.6)\n",
      "Requirement already satisfied: pandas>=0.24.0 in c:\\dev\\miniconda3\\lib\\site-packages (from catboost) (1.4.3)\n",
      "Requirement already satisfied: plotly in c:\\dev\\miniconda3\\lib\\site-packages (from catboost) (5.10.0)\n",
      "Requirement already satisfied: matplotlib in c:\\dev\\miniconda3\\lib\\site-packages (from catboost) (3.5.3)\n",
      "Requirement already satisfied: scipy in c:\\dev\\miniconda3\\lib\\site-packages (from catboost) (1.9.0)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\dev\\miniconda3\\lib\\site-packages (from catboost) (1.23.2)\n",
      "Requirement already satisfied: graphviz in c:\\dev\\miniconda3\\lib\\site-packages (from catboost) (0.20.1)\n",
      "Requirement already satisfied: six in c:\\dev\\miniconda3\\lib\\site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\dev\\miniconda3\\lib\\site-packages (from pandas>=0.24.0->catboost) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\dev\\miniconda3\\lib\\site-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\dev\\miniconda3\\lib\\site-packages (from matplotlib->catboost) (3.0.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\dev\\miniconda3\\lib\\site-packages (from matplotlib->catboost) (9.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\dev\\miniconda3\\lib\\site-packages (from matplotlib->catboost) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\dev\\miniconda3\\lib\\site-packages (from matplotlib->catboost) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\dev\\miniconda3\\lib\\site-packages (from matplotlib->catboost) (4.35.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\dev\\miniconda3\\lib\\site-packages (from matplotlib->catboost) (1.4.4)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\dev\\miniconda3\\lib\\site-packages (from plotly->catboost) (8.0.1)\n",
      "Requirement already satisfied: skranger in c:\\dev\\miniconda3\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0 in c:\\dev\\miniconda3\\lib\\site-packages (from skranger) (1.1.2)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\dev\\miniconda3\\lib\\site-packages (from scikit-learn>=1.0->skranger) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\dev\\miniconda3\\lib\\site-packages (from scikit-learn>=1.0->skranger) (1.23.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\dev\\miniconda3\\lib\\site-packages (from scikit-learn>=1.0->skranger) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\dev\\miniconda3\\lib\\site-packages (from scikit-learn>=1.0->skranger) (1.9.0)\n",
      "Requirement already satisfied: ngboost in c:\\dev\\miniconda3\\lib\\site-packages (0.3.12)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\dev\\miniconda3\\lib\\site-packages (from ngboost) (1.23.2)\n",
      "Requirement already satisfied: lifelines>=0.25 in c:\\dev\\miniconda3\\lib\\site-packages (from ngboost) (0.27.1)\n",
      "Requirement already satisfied: scikit-learn>=0.21 in c:\\dev\\miniconda3\\lib\\site-packages (from ngboost) (1.1.2)\n",
      "Requirement already satisfied: scipy>=1.3 in c:\\dev\\miniconda3\\lib\\site-packages (from ngboost) (1.9.0)\n",
      "Requirement already satisfied: tqdm>=4.3 in c:\\dev\\miniconda3\\lib\\site-packages (from ngboost) (4.63.0)\n",
      "Requirement already satisfied: autograd>=1.3 in c:\\dev\\miniconda3\\lib\\site-packages (from lifelines>=0.25->ngboost) (1.4)\n",
      "Requirement already satisfied: matplotlib>=3.0 in c:\\dev\\miniconda3\\lib\\site-packages (from lifelines>=0.25->ngboost) (3.5.3)\n",
      "Requirement already satisfied: formulaic>=0.2.2 in c:\\dev\\miniconda3\\lib\\site-packages (from lifelines>=0.25->ngboost) (0.4.0)\n",
      "Requirement already satisfied: autograd-gamma>=0.3 in c:\\dev\\miniconda3\\lib\\site-packages (from lifelines>=0.25->ngboost) (0.5.0)\n",
      "Requirement already satisfied: pandas>=1.0.0 in c:\\dev\\miniconda3\\lib\\site-packages (from lifelines>=0.25->ngboost) (1.4.3)\n",
      "Requirement already satisfied: future>=0.15.2 in c:\\dev\\miniconda3\\lib\\site-packages (from autograd>=1.3->lifelines>=0.25->ngboost) (0.18.2)\n",
      "Requirement already satisfied: interface-meta<2.0.0,>=1.2.0 in c:\\dev\\miniconda3\\lib\\site-packages (from formulaic>=0.2.2->lifelines>=0.25->ngboost) (1.3.0)\n",
      "Requirement already satisfied: wrapt>=1.0 in c:\\dev\\miniconda3\\lib\\site-packages (from formulaic>=0.2.2->lifelines>=0.25->ngboost) (1.14.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.2.0 in c:\\dev\\miniconda3\\lib\\site-packages (from formulaic>=0.2.2->lifelines>=0.25->ngboost) (4.3.0)\n",
      "Requirement already satisfied: astor>=0.8 in c:\\dev\\miniconda3\\lib\\site-packages (from formulaic>=0.2.2->lifelines>=0.25->ngboost) (0.8.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\dev\\miniconda3\\lib\\site-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\dev\\miniconda3\\lib\\site-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (4.35.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\dev\\miniconda3\\lib\\site-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\dev\\miniconda3\\lib\\site-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (3.0.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\dev\\miniconda3\\lib\\site-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\dev\\miniconda3\\lib\\site-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\dev\\miniconda3\\lib\\site-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (0.11.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\dev\\miniconda3\\lib\\site-packages (from pandas>=1.0.0->lifelines>=0.25->ngboost) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\dev\\miniconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.0->lifelines>=0.25->ngboost) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\dev\\miniconda3\\lib\\site-packages (from scikit-learn>=0.21->ngboost) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\dev\\miniconda3\\lib\\site-packages (from scikit-learn>=0.21->ngboost) (3.1.0)\n",
      "Requirement already satisfied: colorama in c:\\dev\\miniconda3\\lib\\site-packages (from tqdm>=4.3->ngboost) (0.4.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\dev\\miniconda3\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: wheel in c:\\dev\\miniconda3\\lib\\site-packages (from lightgbm) (0.37.1)\n",
      "Requirement already satisfied: numpy in c:\\dev\\miniconda3\\lib\\site-packages (from lightgbm) (1.23.2)\n",
      "Requirement already satisfied: scipy in c:\\dev\\miniconda3\\lib\\site-packages (from lightgbm) (1.9.0)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\dev\\miniconda3\\lib\\site-packages (from lightgbm) (1.1.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\dev\\miniconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\dev\\miniconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (1.1.0)\n",
      "Requirement already satisfied: hyperopt in c:\\dev\\miniconda3\\lib\\site-packages (0.2.7)\n",
      "Requirement already satisfied: six in c:\\dev\\miniconda3\\lib\\site-packages (from hyperopt) (1.16.0)\n",
      "Requirement already satisfied: future in c:\\dev\\miniconda3\\lib\\site-packages (from hyperopt) (0.18.2)\n",
      "Requirement already satisfied: tqdm in c:\\dev\\miniconda3\\lib\\site-packages (from hyperopt) (4.63.0)\n",
      "Requirement already satisfied: numpy in c:\\dev\\miniconda3\\lib\\site-packages (from hyperopt) (1.23.2)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\dev\\miniconda3\\lib\\site-packages (from hyperopt) (2.8.6)\n",
      "Requirement already satisfied: cloudpickle in c:\\dev\\miniconda3\\lib\\site-packages (from hyperopt) (2.1.0)\n",
      "Requirement already satisfied: scipy in c:\\dev\\miniconda3\\lib\\site-packages (from hyperopt) (1.9.0)\n",
      "Requirement already satisfied: py4j in c:\\dev\\miniconda3\\lib\\site-packages (from hyperopt) (0.10.9.7)\n",
      "Requirement already satisfied: colorama in c:\\dev\\miniconda3\\lib\\site-packages (from tqdm->hyperopt) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "# hyper parameter tuning을 위한 패키지 설치\n",
    "!pip install optuna\n",
    "!pip install catboost\n",
    "!pip install skranger\n",
    "!pip install ngboost\n",
    "!pip install lightgbm\n",
    "!pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "pcG2aSVkkAVz"
   },
   "outputs": [],
   "source": [
    "# 기본 modules\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "# 머신러닝 modules\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from ngboost import NGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import ElasticNet, LinearRegression, Lasso, Ridge\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from skranger.ensemble import RangerForestRegressor\n",
    "from sklearn.neighbors import RadiusNeighborsRegressor\n",
    "from ngboost.scores import LogScore\n",
    "\n",
    "from hyperopt import fmin, hp, tpe\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# 모듈화된 함수 사용\n",
    "import utils.preprocessing as preprocessing\n",
    "import utils.utils as utils\n",
    "import utils.stacking as stk\n",
    "import utils.params as params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FB59rtt-llsC"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3129026271.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [5]\u001b[1;36m\u001b[0m\n\u001b[1;33m    global train_x == train_x.drop(['X_10', 'X_11'], axis = 1) # 결측치 많음\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "utils.seed_everything(utils.Config.seed)\n",
    "\n",
    "train_df = pd.read_csv('Data/train.csv')\n",
    "test_x = pd.read_csv('Data/test.csv')\n",
    "train_x, train_y = preprocessing.dataset_split_X_y(train_df)\n",
    "\n",
    "cols_with_zero_variance = preprocessing.zero_variance(train_x) # 분산이 0 (통과 여부)\n",
    "train_x = train_x.drop(cols_with_zero_variance, axis = 1)\n",
    "test_x = test_x.drop(cols_with_zero_variance, axis = 1)\n",
    "\n",
    "global train_x == train_x.drop(['X_10', 'X_11'], axis = 1) # 결측치 많음\n",
    "test_x = test_x.drop(['X_10', 'X_11'], axis = 1)\n",
    "\n",
    "test_x = test_x.drop('ID', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "- 모델별 개별학습(타겟 Y_01~Y_14) 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                          | 0/200 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "job exception: name 'train_x' is not defined\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                                          | 0/200 [00:00<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Parameter Tunning\u001b[39;00m\n\u001b[0;32m      2\u001b[0m space_lgbm \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m : hp\u001b[38;5;241m.\u001b[39mquniform(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m1500\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: hp\u001b[38;5;241m.\u001b[39mquniform(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m250\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: hp\u001b[38;5;241m.\u001b[39mloguniform(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m0.01\u001b[39m), np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m0.5\u001b[39m)),\n\u001b[0;32m     14\u001b[0m }\n\u001b[1;32m---> 16\u001b[0m best \u001b[38;5;241m=\u001b[39m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlgbm_objective\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mspace_lgbm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43malgo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(best)\n\u001b[0;32m     23\u001b[0m best[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(best[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mC:\\Dev\\miniconda3\\lib\\site-packages\\hyperopt\\fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    583\u001b[0m rval\u001b[38;5;241m.\u001b[39mcatch_eval_exceptions \u001b[38;5;241m=\u001b[39m catch_eval_exceptions\n\u001b[0;32m    585\u001b[0m \u001b[38;5;66;03m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[1;32m--> 586\u001b[0m \u001b[43mrval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexhaust\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_argmin:\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trials\u001b[38;5;241m.\u001b[39mtrials) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mC:\\Dev\\miniconda3\\lib\\site-packages\\hyperopt\\fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexhaust\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    363\u001b[0m     n_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials)\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_done\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_until_done\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mC:\\Dev\\miniconda3\\lib\\site-packages\\hyperopt\\fmin.py:300\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    297\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoll_interval_secs)\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;66;03m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserial_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials_save_file \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mC:\\Dev\\miniconda3\\lib\\site-packages\\hyperopt\\fmin.py:178\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    176\u001b[0m ctrl \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mCtrl(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials, current_trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdomain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctrl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    180\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob exception: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[1;32mC:\\Dev\\miniconda3\\lib\\site-packages\\hyperopt\\base.py:892\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;66;03m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[0;32m    885\u001b[0m     \u001b[38;5;66;03m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[0;32m    886\u001b[0m     \u001b[38;5;66;03m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[0;32m    887\u001b[0m     pyll_rval \u001b[38;5;241m=\u001b[39m pyll\u001b[38;5;241m.\u001b[39mrec_eval(\n\u001b[0;32m    888\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpr,\n\u001b[0;32m    889\u001b[0m         memo\u001b[38;5;241m=\u001b[39mmemo,\n\u001b[0;32m    890\u001b[0m         print_node_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[0;32m    891\u001b[0m     )\n\u001b[1;32m--> 892\u001b[0m     rval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyll_rval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rval, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39mnumber)):\n\u001b[0;32m    895\u001b[0m     dict_rval \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(rval), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: STATUS_OK}\n",
      "File \u001b[1;32mE:\\박창준 문서\\1. 취업준비\\2. Python공부\\4. AI\\20220801 LG AI Research 자율주행 센서의 안테나 성능 예측 AI 경진대회\\utils\\params.py:68\u001b[0m, in \u001b[0;36mlgbm_objective\u001b[1;34m(params)\u001b[0m\n\u001b[0;32m     47\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{:.5f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]),   \n\u001b[0;32m     59\u001b[0m }\n\u001b[0;32m     61\u001b[0m model \u001b[38;5;241m=\u001b[39m LGBMRegressor(\n\u001b[0;32m     62\u001b[0m     n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     63\u001b[0m     random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     64\u001b[0m     verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 68\u001b[0m losses \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m-\u001b[39mcross_val_score(model, \u001b[43mtrain_x\u001b[49m, train_y[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY_01\u001b[39m\u001b[38;5;124m'\u001b[39m], cv\u001b[38;5;241m=\u001b[39mutils\u001b[38;5;241m.\u001b[39mConfig\u001b[38;5;241m.\u001b[39mcv, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;66;03m# cross_val_score : 교차검증\u001b[39;00m\n\u001b[0;32m     69\u001b[0m losses \u001b[38;5;241m=\u001b[39m losses \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39mabs(train_y[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY_01\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNRMSE Loss \u001b[39m\u001b[38;5;132;01m{:.5f}\u001b[39;00m\u001b[38;5;124m params \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(losses\u001b[38;5;241m.\u001b[39mmean(), params))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_x' is not defined"
     ]
    }
   ],
   "source": [
    "# Parameter Tunning\n",
    "space_lgbm = {\n",
    "    'n_estimators' : hp.quniform('n_estimators', 100, 1500, 1),\n",
    "    'max_depth': hp.quniform('max_depth', 5, 250, 1),\n",
    "    'num_leaves': hp.quniform('num_leaves', 20, 200, 5),\n",
    "    'min_child_samples': hp.quniform('min_child_samples', 10, 150, 5),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1.0),\n",
    "    'subsample': hp.uniform('subsample', 0.3, 1.0),\n",
    "    'min_split_gain': hp.uniform('min_split_gain', 0, 0.7),\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 1, 10),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 500),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0, 500),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.5)),\n",
    "}\n",
    "\n",
    "best = fmin(fn = params.lgbm_objective,\n",
    "            space = space_lgbm,\n",
    "            algo = tpe.suggest,\n",
    "            verbose = 10,\n",
    "            max_evals = 200)\n",
    "\n",
    "print(best)\n",
    "best['n_estimators'] = int(best['n_estimators'])\n",
    "best['num_leaves'] = int(best['num_leaves'])\n",
    "best['max_depth'] = int(best['max_depth'])\n",
    "best['min_child_samples'] = int(best['min_child_samples'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                          | 0/200 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "job exception: name 'train_x' is not defined\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                                          | 0/200 [00:00<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Parameter Tunning\u001b[39;00m\n\u001b[0;32m      2\u001b[0m space_catboost \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m : hp\u001b[38;5;241m.\u001b[39mquniform(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m300\u001b[39m, \u001b[38;5;241m50\u001b[39m),\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepth\u001b[39m\u001b[38;5;124m'\u001b[39m: hp\u001b[38;5;241m.\u001b[39mquniform(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdepth\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfold_len_multiplier\u001b[39m\u001b[38;5;124m'\u001b[39m : hp\u001b[38;5;241m.\u001b[39mloguniform(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfold_len_multiplier\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1.01\u001b[39m), np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2.5\u001b[39m)),\n\u001b[0;32m     11\u001b[0m }\n\u001b[1;32m---> 13\u001b[0m best \u001b[38;5;241m=\u001b[39m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat_objective\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mspace_catboost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[43malgo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(best)\n",
      "File \u001b[1;32mC:\\Dev\\miniconda3\\lib\\site-packages\\hyperopt\\fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    583\u001b[0m rval\u001b[38;5;241m.\u001b[39mcatch_eval_exceptions \u001b[38;5;241m=\u001b[39m catch_eval_exceptions\n\u001b[0;32m    585\u001b[0m \u001b[38;5;66;03m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[1;32m--> 586\u001b[0m \u001b[43mrval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexhaust\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_argmin:\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trials\u001b[38;5;241m.\u001b[39mtrials) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mC:\\Dev\\miniconda3\\lib\\site-packages\\hyperopt\\fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexhaust\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    363\u001b[0m     n_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials)\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_done\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_until_done\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mC:\\Dev\\miniconda3\\lib\\site-packages\\hyperopt\\fmin.py:300\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    297\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoll_interval_secs)\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;66;03m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserial_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials_save_file \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mC:\\Dev\\miniconda3\\lib\\site-packages\\hyperopt\\fmin.py:178\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    176\u001b[0m ctrl \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mCtrl(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials, current_trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdomain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctrl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    180\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob exception: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[1;32mC:\\Dev\\miniconda3\\lib\\site-packages\\hyperopt\\base.py:892\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;66;03m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[0;32m    885\u001b[0m     \u001b[38;5;66;03m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[0;32m    886\u001b[0m     \u001b[38;5;66;03m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[0;32m    887\u001b[0m     pyll_rval \u001b[38;5;241m=\u001b[39m pyll\u001b[38;5;241m.\u001b[39mrec_eval(\n\u001b[0;32m    888\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpr,\n\u001b[0;32m    889\u001b[0m         memo\u001b[38;5;241m=\u001b[39mmemo,\n\u001b[0;32m    890\u001b[0m         print_node_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[0;32m    891\u001b[0m     )\n\u001b[1;32m--> 892\u001b[0m     rval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyll_rval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rval, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39mnumber)):\n\u001b[0;32m    895\u001b[0m     dict_rval \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(rval), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: STATUS_OK}\n",
      "File \u001b[1;32mE:\\박창준 문서\\1. 취업준비\\2. Python공부\\4. AI\\20220801 LG AI Research 자율주행 센서의 안테나 성능 예측 AI 경진대회\\utils\\params.py:109\u001b[0m, in \u001b[0;36mcat_objective\u001b[1;34m(params)\u001b[0m\n\u001b[0;32m     92\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepth\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepth\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \n\u001b[0;32m    102\u001b[0m }\n\u001b[0;32m    104\u001b[0m model \u001b[38;5;241m=\u001b[39m CatBoostRegressor(\n\u001b[0;32m    105\u001b[0m     logging_level\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSilent\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    107\u001b[0m )\n\u001b[1;32m--> 109\u001b[0m losses \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m-\u001b[39mcross_val_score(model, \u001b[43mtrain_x\u001b[49m, train_y[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY_01\u001b[39m\u001b[38;5;124m'\u001b[39m], cv\u001b[38;5;241m=\u001b[39mConfig\u001b[38;5;241m.\u001b[39mcv, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m    110\u001b[0m losses \u001b[38;5;241m=\u001b[39m losses \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39mabs(train_y[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY_01\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNRMSE Loss \u001b[39m\u001b[38;5;132;01m{:.5f}\u001b[39;00m\u001b[38;5;124m params \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(losses\u001b[38;5;241m.\u001b[39mmean(), params))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_x' is not defined"
     ]
    }
   ],
   "source": [
    "# Parameter Tunning\n",
    "space_catboost = {\n",
    "    'n_estimators' : hp.quniform('n_estimators', 100, 300, 50),\n",
    "    'depth': hp.quniform(\"depth\", 2, 16, 1),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n",
    "    'l2_leaf_reg': hp.uniform('l2_leaf_reg', 3, 8),\n",
    "    'max_bin' : hp.quniform('max_bin', 1, 254, 1),\n",
    "    'min_data_in_leaf' : hp.quniform('min_data_in_leaf', 2, 700, 1),\n",
    "    'random_strength' : hp.loguniform('random_strength', np.log(0.005), np.log(5)),\n",
    "    'fold_len_multiplier' : hp.loguniform('fold_len_multiplier', np.log(1.01), np.log(2.5)),\n",
    "}\n",
    "\n",
    "best = fmin(fn = params.cat_objective,\n",
    "            space = space_catboost,\n",
    "            algo = tpe.suggest,\n",
    "            verbose = 1,\n",
    "            max_evals = 200)\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Tunning\n",
    "space_extra = {\n",
    "    'n_estimators' : hp.quniform('n_estimators', 100, 1500, 50),\n",
    "    'max_depth': hp.quniform('max_depth', 3, 50, 1),\n",
    "    'min_samples_split': hp.quniform('min_samples_split', 5, 50, 5),\n",
    "    'min_samples_leaf': hp.quniform('min_samples_leaf', 5, 50, 1),\n",
    "    'min_weight_fraction_leaf': hp.uniform('min_weight_fraction_leaf', 0.01, 0.5),\n",
    "    'max_features': hp.choice('max_features', ['sqrt', 'log2', None, 'auto']),\n",
    "    'max_leaf_nodes': hp.quniform('max_leaf_nodes', 3, 30, 1),\n",
    "    'min_impurity_decrease': hp.uniform('min_impurity_decrease', 0, 200),\n",
    "    'bootstrap':  hp.choice('bootstrap', [True, False]),\n",
    "    'ccp_alpha': hp.uniform('ccp_alpha', 0.01, 1.0),\n",
    "}\n",
    "\n",
    "best = fmin(fn = params.extra_objective,\n",
    "            space = space_extra,\n",
    "            algo = tpe.suggest,\n",
    "            verbose = 1,\n",
    "            max_evals = 2)\n",
    "\n",
    "best['n_estimators'] = int(best['n_estimators'])\n",
    "best['max_depth'] = int(best['max_depth'])\n",
    "best['max_leaf_nodes'] = int(best['max_leaf_nodes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NGBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Tunning\n",
    "space_ngboost = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 100, 500, 10),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n",
    "    'natural_gradient': hp.choice('natural_gradient', [True, False]),\n",
    "    'col_sample': hp.quniform('col_sample', 0, 1, 0.01),\n",
    "    'minibatch_frac': hp.quniform('minibatch_frac', 0, 1, 0.01),\n",
    "    'tol': hp.uniform('tol', 1e-6, 3e-4),\n",
    "}\n",
    "\n",
    "best = fmin(fn = params.ngbr_objective,\n",
    "            space = space_ngboost,\n",
    "            algo = tpe.suggest,\n",
    "            verbose = 10,\n",
    "            max_evals = 100)\n",
    "\n",
    "print(best)\n",
    "best['n_estimators'] = int(best['n_estimators'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Ensenble\n",
    "- 모델별 개별학습(타겟 Y_01~Y_14) 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-PxkCnAsG6Q"
   },
   "outputs": [],
   "source": [
    "# Y_01\n",
    "#min_samples_split\n",
    "\n",
    "cat_01 = {'depth': 9, 'fold_len_multiplier': 1.6722688563924544, 'l2_leaf_reg': 9.992348977307927, 'learning_rate': 0.03686783566671033, 'max_bin': 16, 'min_data_in_leaf': 7, 'n_estimators': 500, 'random_strength': 0.5478002316160607}\n",
    "\n",
    "extra_01 = {'bootstrap': 0, 'ccp_alpha': 0.5956203348598316, 'max_depth': 43, 'max_features': 1, 'max_leaf_nodes': 22, 'min_impurity_decrease': 195.59697998782488, 'min_samples_leaf': 0.4, 'min_samples_split': 0.427, 'min_weight_fraction_leaf': 0.3872874854064327, 'n_estimators': 200}\n",
    "\n",
    "lgbm_01 = {'colsample_bytree': 0.572280100273023, 'learning_rate': 0.010283635038627429, 'max_depth': 180, 'min_child_samples': 135, 'min_split_gain': 0.04511227284338413, 'n_estimators': 900, 'num_leaves': 70, 'reg_alpha': 4.406681827912319, 'reg_lambda': 20.4785600448913, 'scale_pos_weight': 8.302374117433086, 'subsample': 0.1688669888026464}\n",
    "\n",
    "ngbr_01 = {'n_estimators': 250, 'learning_rate': 0.027115337704182965, 'natural_gradient': True, 'col_sample': 0.2, 'minibatch_frac': 0.8, 'tol': 5.5136412071576055e-05}\n",
    "\n",
    "xx_train, xx_test = stk.get_stacking_base_datasets('cat', train_x, train_y, col='Y_01', test=test_x, params = cat_01)\n",
    "yy_train, yy_test = stk.get_stacking_base_datasets('extra', train_x, train_y, col='Y_01', test=test_x, params = extra_01)\n",
    "zz_train, zz_test = stk.get_stacking_base_datasets('lgbm', train_x, train_y, col='Y_01', test=test_x, params = lgbm_01)\n",
    "qq_train, qq_test = stk.get_stacking_base_datasets('ngbr', train_x, train_y, col='Y_01', test=test_x, params = ngbr_01)\n",
    "\n",
    "\n",
    "Stack_final_X_train = np.concatenate((xx_train, yy_train, zz_train, qq_train), axis=1)\n",
    "Stack_final_X_test = np.concatenate((xx_test, yy_test, zz_test, qq_test), axis=1)\n",
    "\n",
    "# final_model 선택\n",
    "lr_final = LGBMRegressor(**lgbm_01)\n",
    "lr_final.fit(Stack_final_X_train, train_y['Y_01'])\n",
    "stack_final1 = lr_final.predict(Stack_final_X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Y_02\n",
    "#min_samples_split\n",
    "\n",
    "cat_02 = {'depth': 9, 'fold_len_multiplier': 1.8379420467251593, 'l2_leaf_reg': 27.102344731579784, 'learning_rate': 0.03597820559455176, 'max_bin': 14, 'min_data_in_leaf': 2, 'n_estimators': 500, 'random_strength': 0.9800368067026318}\n",
    "\n",
    "extra_02 = {'bootstrap': 0, 'ccp_alpha': 0.9020983921597531, 'max_depth': 42, 'max_features': 2, 'max_leaf_nodes': 7, 'min_impurity_decrease': 150.7925115966371, 'min_samples_leaf': 18, 'min_samples_split': 15, 'min_weight_fraction_leaf': 0.3321219768527379, 'n_estimators': 200}\n",
    "\n",
    "lgbm_02 =  {'colsample_bytree': 0.7641322280477741, 'learning_rate': 0.010977205425053654, 'max_depth': 90, 'min_child_samples': 75, 'min_split_gain': 0.13379952895779884, 'n_estimators': 900, 'num_leaves': 80, 'reg_alpha': 1.9214119194170154, 'reg_lambda': 14.454450236504218, 'scale_pos_weight': 2.171961031806387, 'subsample': 0.9552593593877317}\n",
    "\n",
    "ngbr_02 = {'n_estimators': 250, 'learning_rate': 0.0656349966273891, 'natural_gradient': True, 'col_sample': 0.8, 'minibatch_frac': 0.55, 'tol': 0.00028029350235701545}\n",
    "\n",
    "xx_train, xx_test = stk.get_stacking_base_datasets('cat', train_x, train_y, col='Y_02', test=test_x, params = cat_02)\n",
    "yy_train, yy_test = stk.get_stacking_base_datasets('extra', train_x, train_y, col='Y_02', test=test_x, params = extra_02)\n",
    "zz_train, zz_test = stk.get_stacking_base_datasets('lgbm', train_x, train_y, col='Y_02', test=test_x, params = lgbm_02)\n",
    "qq_train, qq_test = stk.get_stacking_base_datasets('ngbr', train_x, train_y, col='Y_02', test=test_x, params = ngbr_02)\n",
    "\n",
    "\n",
    "Stack_final_X_train = np.concatenate((xx_train, yy_train, zz_train, qq_train), axis=1)\n",
    "Stack_final_X_test = np.concatenate((xx_test, yy_test, zz_test, qq_test), axis=1)\n",
    "\n",
    "# final_model 선택\n",
    "lr_final = LGBMRegressor(**lgbm_02)\n",
    "lr_final.fit(Stack_final_X_train, train_y['Y_02'])\n",
    "stack_final2 = lr_final.predict(Stack_final_X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Y_03\n",
    "#min_samples_split\n",
    "\n",
    "cat_03 = {'depth': 7, 'fold_len_multiplier': 2.498263900973405, 'l2_leaf_reg': 36.312606304859514, 'learning_rate': 0.043147429358500425, 'max_bin': 14, 'min_data_in_leaf': 2, 'n_estimators': 450, 'random_strength': 0.6650471273750369}\n",
    "\n",
    "extra_03 = {'bootstrap': 1, 'ccp_alpha': 0.9068334703113171, 'max_depth': 22, 'max_features': 2, 'max_leaf_nodes': 11, 'min_impurity_decrease': 6.407983868655598, 'min_samples_leaf': 17, 'min_samples_split': 10, 'min_weight_fraction_leaf': 0.4864839919729328, 'n_estimators': 1450}\n",
    "\n",
    "lgbm_03 = {'colsample_bytree': 0.5504769098255781,  'learning_rate': 0.019653385015120244, 'max_depth': 220, 'min_child_samples': 25, 'min_split_gain': 0.1273611040963466, 'n_estimators': 470, 'num_leaves': 160, 'reg_alpha': 3.5549669150756706, 'reg_lambda': 39.88636182674132, 'scale_pos_weight': 12.46696320152359, 'subsample': 0.7590007450921917}\n",
    "\n",
    "ngbr_03 = {'n_estimators': 410, 'learning_rate': 0.11407060690033853, 'natural_gradient': True, 'col_sample': 0.25, 'minibatch_frac': 1, 'tol': 0.000166350313681024}\n",
    "\n",
    "xx_train, xx_test = stk.get_stacking_base_datasets('cat', train_x, train_y, col='Y_03', test=test_x, params = cat_03)\n",
    "yy_train, yy_test = stk.get_stacking_base_datasets('extra', train_x, train_y, col='Y_03', test=test_x, params = extra_03)\n",
    "zz_train, zz_test = stk.get_stacking_base_datasets('lgbm', train_x, train_y, col='Y_03', test=test_x, params = lgbm_03)\n",
    "qq_train, qq_test = stk.get_stacking_base_datasets('ngbr', train_x, train_y, col='Y_03', test=test_x, params = ngbr_03)\n",
    "\n",
    "\n",
    "Stack_final_X_train = np.concatenate((xx_train, yy_train, zz_train, qq_train), axis=1)\n",
    "Stack_final_X_test = np.concatenate((xx_test, yy_test, zz_test, qq_test), axis=1)\n",
    "\n",
    "# final_model 선택\n",
    "lr_final = LGBMRegressor(**lgbm_03)\n",
    "lr_final.fit(Stack_final_X_train, train_y['Y_03'])\n",
    "stack_final3 = lr_final.predict(Stack_final_X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Y_04\n",
    "#min_samples_split\n",
    "\n",
    "cat_04 = {'depth': 10, 'fold_len_multiplier': 1.3980250309157025, 'l2_leaf_reg': 5.838850864558845, 'learning_rate': 0.2737350830778467, 'max_bin': 78, 'min_data_in_leaf': 561, 'n_estimators': 20, 'random_strength': 0.009672204431612739}\n",
    "\n",
    "extra_04 = {'bootstrap': 0, 'ccp_alpha': 0.7076108565007323, 'max_depth': 48, 'max_features': 3, 'max_leaf_nodes': 16, 'min_impurity_decrease': 45.084212905659186, 'min_samples_leaf': 11, 'min_samples_split': 20, 'min_weight_fraction_leaf': 0.1164511175367393, 'n_estimators': 100}\n",
    "\n",
    "lgbm_04 = {'colsample_bytree': 0.5597537952569402, 'learning_rate': 0.02374663979814546, 'max_depth': 32, 'min_child_samples': 100, 'min_split_gain': 0.12211426885216736, 'n_estimators': 1263, 'num_leaves': 200, 'reg_alpha': 14.606693962963451, 'reg_lambda': 299.52278825209424, 'scale_pos_weight': 7.7785016838070735, 'subsample': 0.6254745287838821}\n",
    "\n",
    "ngbr_04 = {'n_estimators': 431, 'learning_rate': 0.08883519586581468, 'natural_gradient': True, 'col_sample': 0.97, 'minibatch_frac': 0.75, 'tol': 8.896682623929568e-05}\n",
    "\n",
    "xx_train, xx_test = stk.get_stacking_base_datasets('cat', train_x, train_y, col='Y_04', test=test_x, params = cat_04)\n",
    "yy_train, yy_test = stk.get_stacking_base_datasets('extra', train_x, train_y, col='Y_04', test=test_x, params = extra_04)\n",
    "zz_train, zz_test = stk.get_stacking_base_datasets('lgbm', train_x, train_y, col='Y_04', test=test_x, params = lgbm_04)\n",
    "qq_train, qq_test = stk.get_stacking_base_datasets('ngbr', train_x, train_y, col='Y_04', test=test_x, params = ngbr_04)\n",
    "\n",
    "\n",
    "Stack_final_X_train = np.concatenate((xx_train, yy_train, zz_train, qq_train), axis=1)\n",
    "Stack_final_X_test = np.concatenate((xx_test, yy_test, zz_test, qq_test), axis=1)\n",
    "\n",
    "# final_model 선택\n",
    "lr_final = LGBMRegressor(**lgbm_04)\n",
    "lr_final.fit(Stack_final_X_train, train_y['Y_04'])\n",
    "stack_final4 = lr_final.predict(Stack_final_X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Y_05\n",
    "#min_samples_split\n",
    "\n",
    "cat_05 = {'depth': 9, 'fold_len_multiplier': 1.7527609013156893, 'l2_leaf_reg': 5.150371128645829, 'learning_rate': 0.23166991521375363, 'max_bin': 181, 'min_data_in_leaf': 591, 'n_estimators': 17, 'random_strength': 0.08626442162325075}\n",
    "\n",
    "extra_05 = {'bootstrap': 0, 'ccp_alpha': 0.17223432236304015, 'max_depth': 39, 'max_features': 2, 'max_leaf_nodes': 16, 'min_impurity_decrease': 166.70077338146032, 'min_samples_leaf': 34, 'min_samples_split': 15, 'min_weight_fraction_leaf': 0.22146453407955657, 'n_estimators': 200}\n",
    "\n",
    "lgbm_05 = {'colsample_bytree': 0.4311015575880258, 'learning_rate': 0.01749725932551278, 'max_depth': 53, 'min_child_samples': 15, 'min_split_gain': 0.2820951740673634, 'n_estimators': 974, 'num_leaves': 165, 'reg_alpha': 9.604623064885754, 'reg_lambda': 12.314490508636432, 'scale_pos_weight': 6.6422956907936825, 'subsample': 0.7390190399971659}\n",
    "\n",
    "ngbr_05 = {'n_estimators': 449, 'learning_rate': 0.08850472848590257, 'natural_gradient': True, 'col_sample': 0.8, 'minibatch_frac': 0.97, 'tol': 7.048508838263751e-05}\n",
    "\n",
    "xx_train, xx_test = stk.get_stacking_base_datasets('cat', train_x, train_y, col='Y_05', test=test_x, params = cat_05)\n",
    "yy_train, yy_test = stk.get_stacking_base_datasets('extra', train_x, train_y, col='Y_05', test=test_x, params = extra_05)\n",
    "zz_train, zz_test = stk.get_stacking_base_datasets('lgbm', train_x, train_y, col='Y_05', test=test_x, params = lgbm_05)\n",
    "qq_train, qq_test = stk.get_stacking_base_datasets('ngbr', train_x, train_y, col='Y_05', test=test_x, params = ngbr_05)\n",
    "\n",
    "\n",
    "Stack_final_X_train = np.concatenate((xx_train, yy_train, zz_train, qq_train), axis=1)\n",
    "Stack_final_X_test = np.concatenate((xx_test, yy_test, zz_test, qq_test), axis=1)\n",
    "\n",
    "# final_model 선택\n",
    "lr_final = LGBMRegressor(**lgbm_05)\n",
    "lr_final.fit(Stack_final_X_train, train_y['Y_05'])\n",
    "stack_final5 = lr_final.predict(Stack_final_X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Y_06\n",
    "#min_samples_split\n",
    "\n",
    "cat_06 = {'depth': 8, 'fold_len_multiplier': 1.4734463589684192, 'l2_leaf_reg': 7.343561976614034, 'learning_rate': 0.2546701358021111, 'max_bin': 8, 'min_data_in_leaf': 280, 'n_estimators': 13, 'random_strength': 0.06423202371274109}\n",
    "\n",
    "extra_06 = {'bootstrap': 0, 'ccp_alpha': 0.263122350467869, 'max_depth': 12, 'max_features': 1, 'max_leaf_nodes': 6, 'min_impurity_decrease': 140.20905071348278, 'min_samples_leaf': 50, 'min_samples_split': 50, 'min_weight_fraction_leaf': 0.17986464144348094, 'n_estimators': 450}\n",
    "\n",
    "lgbm_06 = {'colsample_bytree': 0.6889745043181079, 'learning_rate': 0.06146161938790444, 'max_depth': 89, 'min_child_samples': 10, 'min_split_gain': 0.669592868575692, 'n_estimators': 1169, 'num_leaves': 175, 'reg_alpha': 11.405277636150856, 'reg_lambda': 112.37954230084294, 'scale_pos_weight': 5.932435783263877, 'subsample': 0.8265223228903998}\n",
    "\n",
    "ngbr_06 = {'n_estimators': 153, 'learning_rate': 0.05121743231435252, 'natural_gradient': True, 'col_sample': 0.7000000000000001, 'minibatch_frac': 0.97, 'tol': 0.00013297533787653073}\n",
    "\n",
    "xx_train, xx_test = stk.get_stacking_base_datasets('cat', train_x, train_y, col='Y_06', test=test_x, params = cat_06)\n",
    "yy_train, yy_test = stk.get_stacking_base_datasets('extra', train_x, train_y, col='Y_06', test=test_x, params = extra_06)\n",
    "zz_train, zz_test = stk.get_stacking_base_datasets('lgbm', train_x, train_y, col='Y_06', test=test_x, params = lgbm_06)\n",
    "qq_train, qq_test = stk.get_stacking_base_datasets('ngbr', train_x, train_y, col='Y_06', test=test_x, params = ngbr_06)\n",
    "\n",
    "\n",
    "Stack_final_X_train = np.concatenate((xx_train, yy_train, zz_train, qq_train), axis=1)\n",
    "Stack_final_X_test = np.concatenate((xx_test, yy_test, zz_test, qq_test), axis=1)\n",
    "\n",
    "# final_model 선택\n",
    "lr_final = LGBMRegressor(**lgbm_06)\n",
    "lr_final.fit(Stack_final_X_train, train_y['Y_06'])\n",
    "stack_final6 = lr_final.predict(Stack_final_X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Y_07\n",
    "#min_samples_split\n",
    "\n",
    "cat_07 = {'n_estimators': 350, 'depth': 9, 'learning_rate': 0.07700980062937977, 'l2_leaf_reg': 7.366594895982364, 'max_bin': 80, 'min_data_in_leaf': 654, 'random_strength': 0.12106590929604114, 'fold_len_multiplier': 2.3574644740356874}\n",
    "\n",
    "extra_07 = {'n_estimators': 100, 'max_depth': 9, 'min_samples_split': 15, 'min_samples_leaf': 7, 'min_weight_fraction_leaf': 0.15163758513069608, 'max_features': 'auto', 'max_leaf_nodes': 16, 'min_impurity_decrease': 28.474990166012663, 'bootstrap': True, 'ccp_alpha': 0.8351211481986806}\n",
    "\n",
    "lgbm_07 = {'colsample_bytree': 0.8663251864650988, 'learning_rate': 0.018110306887688978, 'max_depth': 166, 'min_child_samples': 50, 'min_split_gain': 0.025403061552667243, 'n_estimators': 1080, 'num_leaves': 100, 'reg_alpha': 2.0131018839563666, 'reg_lambda': 63.56640846106552, 'scale_pos_weight': 1.8584564419776715, 'subsample': 0.7643028435523616}\n",
    "\n",
    "ngbr_07 = {'n_estimators': 280, 'learning_rate': 0.0543058099317307, 'natural_gradient': False, 'col_sample': 0.61, 'minibatch_frac': 0.53, 'tol': 4.259736507913848e-06}\n",
    "\n",
    "xx_train, xx_test = stk.get_stacking_base_datasets('cat', train_x, train_y, col='Y_07', test=test_x, params = cat_07)\n",
    "yy_train, yy_test = stk.get_stacking_base_datasets('extra', train_x, train_y, col='Y_07', test=test_x, params = extra_07)\n",
    "zz_train, zz_test = stk.get_stacking_base_datasets('lgbm', train_x, train_y, col='Y_07', test=test_x, params = lgbm_07)\n",
    "qq_train, qq_test = stk.get_stacking_base_datasets('ngbr', train_x, train_y, col='Y_07', test=test_x, params = ngbr_07)\n",
    "\n",
    "\n",
    "Stack_final_X_train = np.concatenate((xx_train, yy_train, zz_train, qq_train), axis=1)\n",
    "Stack_final_X_test = np.concatenate((xx_test, yy_test, zz_test, qq_test), axis=1)\n",
    "\n",
    "# final_model 선택\n",
    "lr_final = LGBMRegressor(**lgbm_07)\n",
    "lr_final.fit(Stack_final_X_train, train_y['Y_07'])\n",
    "stack_final7 = lr_final.predict(Stack_final_X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Y_08\n",
    "#min_samples_split\n",
    "\n",
    "cat_08 = {'n_estimators': 450, 'depth': 11, 'learning_rate': 0.06098987016322603, 'l2_leaf_reg': 7.78479685048665, 'max_bin': 244, 'min_data_in_leaf': 423, 'random_strength': 0.4646435148235979, 'fold_len_multiplier': 1.7857138740606202}\n",
    "\n",
    "extra_08 = {'n_estimators': 150, 'max_depth': 31, 'min_samples_split': 45, 'min_samples_leaf': 38, 'min_weight_fraction_leaf': 0.3764820550107259, 'max_features': 'log2', 'max_leaf_nodes': 21, 'min_impurity_decrease': 41.54716726233874, 'bootstrap': True, 'ccp_alpha': 0.9985147070627858}\n",
    "\n",
    "lgbm_08 = {'colsample_bytree': 0.8970390757241629, 'learning_rate': 0.03571726260659087, 'max_depth': 164, 'min_child_samples': 30, 'min_split_gain': 0.2863362850926679, 'n_estimators': 740, 'num_leaves': 100, 'reg_alpha': 1.1167159754886287, 'reg_lambda': 280.9798636389436, 'scale_pos_weight': 4.75867892931176, 'subsample': 0.681716202670263}\n",
    "\n",
    "ngbr_08 = {'n_estimators': 490, 'learning_rate': 0.04770929499260395, 'natural_gradient': True, 'col_sample': 1, 'minibatch_frac': 0.6, 'tol': 0.00019175990157775972}\n",
    "\n",
    "xx_train, xx_test = stk.get_stacking_base_datasets('cat', train_x, train_y, col='Y_08', test=test_x, params = cat_08)\n",
    "yy_train, yy_test = stk.get_stacking_base_datasets('extra', train_x, train_y, col='Y_08', test=test_x, params = extra_08)\n",
    "zz_train, zz_test = stk.get_stacking_base_datasets('lgbm', train_x, train_y, col='Y_08', test=test_x, params = lgbm_08)\n",
    "qq_train, qq_test = stk.get_stacking_base_datasets('ngbr', train_x, train_y, col='Y_08', test=test_x, params = ngbr_08)\n",
    "\n",
    "\n",
    "Stack_final_X_train = np.concatenate((xx_train, yy_train, zz_train, qq_train), axis=1)\n",
    "Stack_final_X_test = np.concatenate((xx_test, yy_test, zz_test, qq_test), axis=1)\n",
    "\n",
    "# final_model 선택\n",
    "lr_final = LGBMRegressor(**lgbm_08)\n",
    "lr_final.fit(Stack_final_X_train, train_y['Y_08'])\n",
    "stack_final8 = lr_final.predict(Stack_final_X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Y_09\n",
    "#min_samples_split\n",
    "\n",
    "cat_09 = {'n_estimators': 450, 'depth': 7, 'learning_rate': 0.08417441728687995, 'l2_leaf_reg': 7.003716950710109, 'max_bin': 249, 'min_data_in_leaf': 85, 'random_strength': 0.01346018468768982, 'fold_len_multiplier': 1.8510933222341048}\n",
    "\n",
    "extra_09 = {'n_estimators': 50, 'max_depth': 35, 'min_samples_split': 45, 'min_samples_leaf': 11, 'min_weight_fraction_leaf': 0.1446169507139342, 'max_features': None, 'max_leaf_nodes': 10, 'min_impurity_decrease': 190.68425357881898, 'bootstrap': True, 'ccp_alpha': 0.8503455224547213}\n",
    "\n",
    "lgbm_09 = {'n_estimators': 900, 'max_depth': 86, 'num_leaves': 150, 'min_child_samples': 85, 'colsample_bytree': '0.90507', 'subsample': '0.62362', 'min_split_gain': '0.21034', 'scale_pos_weight': '8.77311', 'reg_alpha': '0.07069', 'reg_lambda': '499.10672', 'learning_rate': '0.04679'}\n",
    "\n",
    "ngbr_09 = {'n_estimators': 250, 'learning_rate': 0.0531679007741611, 'natural_gradient': False, 'col_sample': 0.8200000000000001, 'minibatch_frac': 0.65, 'tol': 0.00013547208288125422}\n",
    "\n",
    "xx_train, xx_test = stk.get_stacking_base_datasets('cat', train_x, train_y, col='Y_09', test=test_x, params = cat_09)\n",
    "yy_train, yy_test = stk.get_stacking_base_datasets('extra', train_x, train_y, col='Y_09', test=test_x, params = extra_09)\n",
    "zz_train, zz_test = stk.get_stacking_base_datasets('lgbm', train_x, train_y, col='Y_09', test=test_x, params = lgbm_09)\n",
    "qq_train, qq_test = stk.get_stacking_base_datasets('ngbr', train_x, train_y, col='Y_09', test=test_x, params = ngbr_09)\n",
    "\n",
    "\n",
    "Stack_final_X_train = np.concatenate((xx_train, yy_train, zz_train, qq_train), axis=1)\n",
    "Stack_final_X_test = np.concatenate((xx_test, yy_test, zz_test, qq_test), axis=1)\n",
    "\n",
    "# final_model 선택\n",
    "lr_final = LGBMRegressor(**lgbm_09)\n",
    "lr_final.fit(Stack_final_X_train, train_y['Y_09'])\n",
    "stack_final9 = lr_final.predict(Stack_final_X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Y_10\n",
    "#min_samples_split\n",
    "\n",
    "cat_10 = {'depth': 10, 'fold_len_multiplier': 1.8964686014236263, 'l2_leaf_reg': 6.890506058346803, 'learning_rate': 0.04431783808011194, 'max_bin': 135, 'min_data_in_leaf': 449, 'n_estimators': 400, 'random_strength': 0.0715293516760773}\n",
    "\n",
    "extra_10 = {'bootstrap': 0, 'ccp_alpha': 0.06816154953537701, 'max_depth': 34, 'max_features': 2, 'max_leaf_nodes': 17, 'min_impurity_decrease': 10.897433724670414, 'min_samples_leaf': 45, 'min_samples_split': 25, 'min_weight_fraction_leaf': 0.22786627091058692, 'n_estimators': 100}\n",
    "\n",
    "lgbm_10 = {'colsample_bytree': 0.8350973419202665, 'learning_rate': 0.03134966396365972, 'max_depth': 114, 'min_child_samples': 20, 'min_split_gain': 0.24406788869557822, 'n_estimators': 454, 'num_leaves': 115, 'reg_alpha': 1.0870546166564243, 'reg_lambda': 346.21163772786895, 'scale_pos_weight': 5.81617865285278, 'subsample': 0.45612075761336973}\n",
    "\n",
    "ngbr_10 = {'n_estimators': 400, 'learning_rate': 0.10343505554950799, 'natural_gradient': True, 'col_sample': 0.71, 'minibatch_frac': 0.96, 'tol': 6.075001314642106e-05}\n",
    "\n",
    "xx_train, xx_test = stk.get_stacking_base_datasets('cat', train_x, train_y, col='Y_10', test=test_x, params = cat_10)\n",
    "yy_train, yy_test = stk.get_stacking_base_datasets('extra', train_x, train_y, col='Y_10', test=test_x, params = extra_10)\n",
    "zz_train, zz_test = stk.get_stacking_base_datasets('lgbm', train_x, train_y, col='Y_10', test=test_x, params = lgbm_10)\n",
    "qq_train, qq_test = stk.get_stacking_base_datasets('ngbr', train_x, train_y, col='Y_10', test=test_x, params = ngbr_10)\n",
    "\n",
    "\n",
    "Stack_final_X_train = np.concatenate((xx_train, yy_train, zz_train, qq_train), axis=1)\n",
    "Stack_final_X_test = np.concatenate((xx_test, yy_test, zz_test, qq_test), axis=1)\n",
    "\n",
    "# final_model 선택\n",
    "lr_final = LGBMRegressor(**lgbm_10)\n",
    "lr_final.fit(Stack_final_X_train, train_y['Y_10'])\n",
    "stack_final10 = lr_final.predict(Stack_final_X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Y_11\n",
    "#min_samples_split\n",
    "\n",
    "cat_11 = {'depth': 9, 'fold_len_multiplier': 1.469922031736939, 'l2_leaf_reg': 5.365254126430433, 'learning_rate': 0.05182013518976086, 'max_bin': 147, 'min_data_in_leaf': 238, 'n_estimators': 450, 'random_strength': 0.07839813420603847}\n",
    "\n",
    "extra_11 = {'bootstrap': 0, 'ccp_alpha': 0.22748332620407474, 'max_depth': 18, 'max_features': 2, 'max_leaf_nodes': 30, 'min_impurity_decrease': 63.03156125087142, 'min_samples_leaf': 6, 'min_samples_split': 20, 'min_weight_fraction_leaf': 0.04105676583350759, 'n_estimators': 150}\n",
    "\n",
    "lgbm_11 = {'colsample_bytree': 0.7285829045071064, 'learning_rate': 0.019839273085108612, 'max_depth': 71, 'min_child_samples': 50, 'min_split_gain': 0.35567737788276876, 'n_estimators': 970, 'num_leaves': 140, 'reg_alpha': 0.27353134227182774, 'reg_lambda': 157.85749037224548, 'scale_pos_weight': 5.956126991298146, 'subsample': 0.7509931500532172}\n",
    "\n",
    "ngbr_11 = {'n_estimators': 330, 'learning_rate': 0.10650808882845716, 'natural_gradient': False, 'col_sample': 0.67, 'minibatch_frac': 0.8200000000000001, 'tol': 5.808799526610105e-05}\n",
    "\n",
    "xx_train, xx_test = stk.get_stacking_base_datasets('cat', train_x, train_y, col='Y_11', test=test_x, params = cat_11)\n",
    "yy_train, yy_test = stk.get_stacking_base_datasets('extra', train_x, train_y, col='Y_11', test=test_x, params = extra_11)\n",
    "zz_train, zz_test = stk.get_stacking_base_datasets('lgbm', train_x, train_y, col='Y_11', test=test_x, params = lgbm_11)\n",
    "qq_train, qq_test = stk.get_stacking_base_datasets('ngbr', train_x, train_y, col='Y_11', test=test_x, params = ngbr_11)\n",
    "\n",
    "\n",
    "Stack_final_X_train = np.concatenate((xx_train, yy_train, zz_train, qq_train), axis=1)\n",
    "Stack_final_X_test = np.concatenate((xx_test, yy_test, zz_test, qq_test), axis=1)\n",
    "\n",
    "# final_model 선택\n",
    "lr_final = LGBMRegressor(**lgbm_11)\n",
    "lr_final.fit(Stack_final_X_train, train_y['Y_11'])\n",
    "stack_final11 = lr_final.predict(Stack_final_X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Y_12\n",
    "#min_samples_split\n",
    "\n",
    "cat_12 = {'n_estimators': 250, 'depth': 6, 'learning_rate': 0.15612168413836122, 'l2_leaf_reg': 4.521702132180398, 'max_bin': 249, 'min_data_in_leaf': 218, 'random_strength': 4.051962608010968, 'fold_len_multiplier': 1.1690440537893567}\n",
    "\n",
    "extra_12 = {'bootstrap': 0, 'ccp_alpha': 0.7514591922993081, 'max_depth': 50, 'max_features': 3, 'max_leaf_nodes': 3, 'min_impurity_decrease': 112.61937027705285, 'min_samples_leaf': 25, 'min_samples_split': 30, 'min_weight_fraction_leaf': 0.16488737437051704, 'n_estimators': 100}\n",
    "\n",
    "lgbm_12 = {'colsample_bytree': 0.6115826698158419, 'learning_rate': 0.010052927231718068, 'max_depth': 71, 'min_child_samples': 85, 'min_split_gain': 0.12003011548878659, 'n_estimators': 1300, 'num_leaves': 120, 'reg_alpha': 1.3013867029804251, 'reg_lambda': 269.3915696845848, 'scale_pos_weight': 5.290961082236748, 'subsample': 0.7542724715058367}\n",
    "\n",
    "ngbr_12 = {'col_sample': 0.79, 'learning_rate': 0.11842716149209648, 'minibatch_frac': 0.78, 'n_estimators': 100, 'natural_gradient': 1, 'tol': 0.00017488848399995865}\n",
    "\n",
    "xx_train, xx_test = stk.get_stacking_base_datasets('cat', train_x, train_y, col='Y_12', test=test_x, params = cat_12)\n",
    "yy_train, yy_test = stk.get_stacking_base_datasets('extra', train_x, train_y, col='Y_12', test=test_x, params = extra_12)\n",
    "zz_train, zz_test = stk.get_stacking_base_datasets('lgbm', train_x, train_y, col='Y_12', test=test_x, params = lgbm_12)\n",
    "qq_train, qq_test = stk.get_stacking_base_datasets('ngbr', train_x, train_y, col='Y_12', test=test_x, params = ngbr_12)\n",
    "\n",
    "\n",
    "Stack_final_X_train = np.concatenate((xx_train, yy_train, zz_train, qq_train), axis=1)\n",
    "Stack_final_X_test = np.concatenate((xx_test, yy_test, zz_test, qq_test), axis=1)\n",
    "\n",
    "# final_model 선택\n",
    "lr_final = LGBMRegressor(**lgbm_12)\n",
    "lr_final.fit(Stack_final_X_train, train_y['Y_12'])\n",
    "stack_final12 = lr_final.predict(Stack_final_X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Y_13\n",
    "#min_samples_split\n",
    "\n",
    "cat_13 = {'n_estimators': 250, 'depth': 10, 'learning_rate': 0.06019223910388208, 'l2_leaf_reg': 3.692745142783531, 'max_bin': 30, 'min_data_in_leaf': 71, 'random_strength': 0.061877335687993515, 'fold_len_multiplier': 2.1969765304562054}\n",
    "\n",
    "extra_13 = {'bootstrap': 0, 'ccp_alpha': 0.9305876780539839, 'max_depth': 32, 'max_features': 3, 'max_leaf_nodes': 6, 'min_impurity_decrease': 103.59182427685685, 'min_samples_leaf': 41, 'min_samples_split': 10, 'min_weight_fraction_leaf': 0.2746586014429824, 'n_estimators': 100}\n",
    "\n",
    "lgbm_13 = {'colsample_bytree': 0.9511047907962863, 'learning_rate': 0.023257873709858216, 'max_depth': 58, 'min_child_samples': 80, 'min_split_gain': 0.21488153574891886, 'n_estimators': 1300, 'num_leaves': 150, 'reg_alpha': 0.33761852089148814, 'reg_lambda': 57.05291849099506, 'scale_pos_weight': 2.0801436555772854, 'subsample': 0.5580106548214563}\n",
    "\n",
    "ngbr_13 = {'n_estimators': 100, 'learning_rate': 0.09415734373605988, 'natural_gradient': False, 'col_sample': 0.67, 'minibatch_frac': 0.86, 'tol': 0.00010625760957734133}\n",
    "\n",
    "xx_train, xx_test = stk.get_stacking_base_datasets('cat', train_x, train_y, col='Y_13', test=test_x, params = cat_13)\n",
    "yy_train, yy_test = stk.get_stacking_base_datasets('extra', train_x, train_y, col='Y_13', test=test_x, params = extra_13)\n",
    "zz_train, zz_test = stk.get_stacking_base_datasets('lgbm', train_x, train_y, col='Y_13', test=test_x, params = lgbm_13)\n",
    "qq_train, qq_test = stk.get_stacking_base_datasets('ngbr', train_x, train_y, col='Y_13', test=test_x, params = ngbr_13)\n",
    "\n",
    "\n",
    "Stack_final_X_train = np.concatenate((xx_train, yy_train, zz_train, qq_train), axis=1)\n",
    "Stack_final_X_test = np.concatenate((xx_test, yy_test, zz_test, qq_test), axis=1)\n",
    "\n",
    "# final_model 선택\n",
    "lr_final = LGBMRegressor(**lgbm_13)\n",
    "lr_final.fit(Stack_final_X_train, train_y['Y_13'])\n",
    "stack_final13 = lr_final.predict(Stack_final_X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Y_14\n",
    "#min_samples_split\n",
    "\n",
    "cat_14 = {'n_estimators': 250, 'depth': 9, 'learning_rate': 0.0994089124916012, 'l2_leaf_reg': 7.430077445061214, 'max_bin': 163, 'min_data_in_leaf': 420, 'random_strength': 0.5517727972303491, 'fold_len_multiplier': 1.1173814975860605}\n",
    "\n",
    "extra_14 = {'bootstrap': 0, 'ccp_alpha': 0.5803551586810121, 'max_depth': 9, 'max_features': 0, 'max_leaf_nodes': 24, 'min_impurity_decrease': 114.7107697231918, 'min_samples_leaf': 6, 'min_samples_split': 35, 'min_weight_fraction_leaf': 0.1492044083631023, 'n_estimators': 150}\n",
    "\n",
    "lgbm_14 = {'colsample_bytree': 0.8851122740930837, 'learning_rate': 0.013136814152245062, 'max_depth': 249, 'min_child_samples': 65, 'min_split_gain': 0.2072264172906347, 'n_estimators': 450, 'num_leaves': 135, 'reg_alpha': 0.642890771203696, 'reg_lambda': 45.624663648443345, 'scale_pos_weight': 6.400746088779947, 'subsample': 0.30084274480143686}\n",
    "\n",
    "ngbr_14 = {'n_estimators': 300, 'learning_rate': 0.08509952436476127, 'natural_gradient': False, 'col_sample': 1, 'minibatch_frac': 0.86, 'tol': 0.00011282289882632527}\n",
    "\n",
    "xx_train, xx_test = stk.get_stacking_base_datasets('cat', train_x, train_y, col='Y_14', test=test_x, params = cat_14)\n",
    "yy_train, yy_test = stk.get_stacking_base_datasets('extra', train_x, train_y, col='Y_14', test=test_x, params = extra_14)\n",
    "zz_train, zz_test = stk.get_stacking_base_datasets('lgbm', train_x, train_y, col='Y_14', test=test_x, params = lgbm_14)\n",
    "qq_train, qq_test = stk.get_stacking_base_datasets('ngbr', train_x, train_y, col='Y_14', test=test_x, params = ngbr_14)\n",
    "\n",
    "\n",
    "Stack_final_X_train = np.concatenate((xx_train, yy_train, zz_train, qq_train), axis=1)\n",
    "Stack_final_X_test = np.concatenate((xx_test, yy_test, zz_test, qq_test), axis=1)\n",
    "\n",
    "# final_model 선택\n",
    "lr_final = LGBMRegressor(**lgbm_14)\n",
    "lr_final.fit(Stack_final_X_train, train_y['Y_14'])\n",
    "stack_final14 = lr_final.predict(Stack_final_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Save(to CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5DRbK4etsHG0"
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('./sample_submission.csv')\n",
    "sub['Y_01'] = stack_final1\n",
    "sub['Y_02'] = stack_final2\n",
    "sub['Y_03'] = stack_final3\n",
    "sub['Y_04'] = stack_final4\n",
    "sub['Y_05'] = stack_final5\n",
    "sub['Y_06'] = stack_final6\n",
    "sub['Y_07'] = stack_final7\n",
    "sub['Y_08'] = stack_final8\n",
    "sub['Y_09'] = stack_final9\n",
    "sub['Y_10'] = stack_final10\n",
    "sub['Y_11'] = stack_final11\n",
    "sub['Y_12'] = stack_final12\n",
    "sub['Y_13'] = stack_final13\n",
    "sub['Y_14'] = stack_final14\n",
    "sub.to_csv('./stack_.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "REAL FINAL.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
