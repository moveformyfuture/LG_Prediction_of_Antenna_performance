{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h6h89BPK1CaA",
    "outputId": "a8843b6a-2099-4d0f-a975-dfaec0a0dd94"
   },
   "source": [
    "# Stacking Ensenble을 활용한 자율주행 센서 안테나 성능예측\n",
    "> 팀명 : 될때까지간다리\n",
    ">\n",
    "> 작성일 : '22.08.31\n",
    ">\n",
    "> 개발환경 : Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development Environment Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V1s0gwOEkAYi",
    "outputId": "929c6c3e-2a40-49e6-8944-331578af424a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in c:\\dev\\miniconda3\\lib\\site-packages (2.10.1)\n",
      "Requirement already satisfied: cmaes>=0.8.2 in c:\\dev\\miniconda3\\lib\\site-packages (from optuna) (0.8.2)\n",
      "Requirement already satisfied: numpy in c:\\dev\\miniconda3\\lib\\site-packages (from optuna) (1.23.2)\n",
      "Requirement already satisfied: tqdm in c:\\dev\\miniconda3\\lib\\site-packages (from optuna) (4.63.0)\n",
      "Requirement already satisfied: PyYAML in c:\\dev\\miniconda3\\lib\\site-packages (from optuna) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\dev\\miniconda3\\lib\\site-packages (from optuna) (21.3)\n",
      "Requirement already satisfied: sqlalchemy>=1.1.0 in c:\\dev\\miniconda3\\lib\\site-packages (from optuna) (1.4.40)\n",
      "Requirement already satisfied: scipy!=1.4.0 in c:\\dev\\miniconda3\\lib\\site-packages (from optuna) (1.9.0)\n",
      "Requirement already satisfied: alembic in c:\\dev\\miniconda3\\lib\\site-packages (from optuna) (1.8.1)\n",
      "Requirement already satisfied: cliff in c:\\dev\\miniconda3\\lib\\site-packages (from optuna) (4.0.0)\n",
      "Requirement already satisfied: colorlog in c:\\dev\\miniconda3\\lib\\site-packages (from optuna) (6.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\dev\\miniconda3\\lib\\site-packages (from packaging>=20.0->optuna) (3.0.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\dev\\miniconda3\\lib\\site-packages (from sqlalchemy>=1.1.0->optuna) (1.1.2)\n",
      "Requirement already satisfied: Mako in c:\\dev\\miniconda3\\lib\\site-packages (from alembic->optuna) (1.2.1)\n",
      "Requirement already satisfied: stevedore>=2.0.1 in c:\\dev\\miniconda3\\lib\\site-packages (from cliff->optuna) (4.0.0)\n",
      "Requirement already satisfied: PrettyTable>=0.7.2 in c:\\dev\\miniconda3\\lib\\site-packages (from cliff->optuna) (3.3.0)\n",
      "Requirement already satisfied: autopage>=0.4.0 in c:\\dev\\miniconda3\\lib\\site-packages (from cliff->optuna) (0.5.1)\n",
      "Requirement already satisfied: cmd2>=1.0.0 in c:\\dev\\miniconda3\\lib\\site-packages (from cliff->optuna) (2.4.2)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\dev\\miniconda3\\lib\\site-packages (from cliff->optuna) (4.12.0)\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in c:\\dev\\miniconda3\\lib\\site-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
      "Requirement already satisfied: pyperclip>=1.6 in c:\\dev\\miniconda3\\lib\\site-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\n",
      "Requirement already satisfied: pyreadline3 in c:\\dev\\miniconda3\\lib\\site-packages (from cmd2>=1.0.0->cliff->optuna) (3.4.1)\n",
      "Requirement already satisfied: attrs>=16.3.0 in c:\\dev\\miniconda3\\lib\\site-packages (from cmd2>=1.0.0->cliff->optuna) (21.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\dev\\miniconda3\\lib\\site-packages (from importlib-metadata>=4.4->cliff->optuna) (3.8.1)\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in c:\\dev\\miniconda3\\lib\\site-packages (from stevedore>=2.0.1->cliff->optuna) (5.10.0)\n",
      "Requirement already satisfied: colorama in c:\\dev\\miniconda3\\lib\\site-packages (from colorlog->optuna) (0.4.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\dev\\miniconda3\\lib\\site-packages (from Mako->alembic->optuna) (2.1.1)\n",
      "Requirement already satisfied: catboost in c:\\dev\\miniconda3\\lib\\site-packages (1.0.6)\n",
      "Requirement already satisfied: six in c:\\dev\\miniconda3\\lib\\site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: matplotlib in c:\\dev\\miniconda3\\lib\\site-packages (from catboost) (3.5.3)\n",
      "Requirement already satisfied: plotly in c:\\dev\\miniconda3\\lib\\site-packages (from catboost) (5.10.0)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\dev\\miniconda3\\lib\\site-packages (from catboost) (1.23.2)\n",
      "Requirement already satisfied: scipy in c:\\dev\\miniconda3\\lib\\site-packages (from catboost) (1.9.0)\n",
      "Requirement already satisfied: graphviz in c:\\dev\\miniconda3\\lib\\site-packages (from catboost) (0.20.1)\n",
      "Requirement already satisfied: pandas>=0.24.0 in c:\\dev\\miniconda3\\lib\\site-packages (from catboost) (1.4.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\dev\\miniconda3\\lib\\site-packages (from pandas>=0.24.0->catboost) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\dev\\miniconda3\\lib\\site-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\dev\\miniconda3\\lib\\site-packages (from matplotlib->catboost) (9.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\dev\\miniconda3\\lib\\site-packages (from matplotlib->catboost) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\dev\\miniconda3\\lib\\site-packages (from matplotlib->catboost) (4.35.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\dev\\miniconda3\\lib\\site-packages (from matplotlib->catboost) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\dev\\miniconda3\\lib\\site-packages (from matplotlib->catboost) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\dev\\miniconda3\\lib\\site-packages (from matplotlib->catboost) (3.0.4)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\dev\\miniconda3\\lib\\site-packages (from plotly->catboost) (8.0.1)\n",
      "Requirement already satisfied: skranger in c:\\dev\\miniconda3\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0 in c:\\dev\\miniconda3\\lib\\site-packages (from skranger) (1.1.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\dev\\miniconda3\\lib\\site-packages (from scikit-learn>=1.0->skranger) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\dev\\miniconda3\\lib\\site-packages (from scikit-learn>=1.0->skranger) (1.9.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\dev\\miniconda3\\lib\\site-packages (from scikit-learn>=1.0->skranger) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\dev\\miniconda3\\lib\\site-packages (from scikit-learn>=1.0->skranger) (1.23.2)\n",
      "Requirement already satisfied: ngboost in c:\\dev\\miniconda3\\lib\\site-packages (0.3.12)\n",
      "Requirement already satisfied: scipy>=1.3 in c:\\dev\\miniconda3\\lib\\site-packages (from ngboost) (1.9.0)\n",
      "Requirement already satisfied: tqdm>=4.3 in c:\\dev\\miniconda3\\lib\\site-packages (from ngboost) (4.63.0)\n",
      "Requirement already satisfied: lifelines>=0.25 in c:\\dev\\miniconda3\\lib\\site-packages (from ngboost) (0.27.1)\n",
      "Requirement already satisfied: scikit-learn>=0.21 in c:\\dev\\miniconda3\\lib\\site-packages (from ngboost) (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\dev\\miniconda3\\lib\\site-packages (from ngboost) (1.23.2)\n",
      "Requirement already satisfied: pandas>=1.0.0 in c:\\dev\\miniconda3\\lib\\site-packages (from lifelines>=0.25->ngboost) (1.4.3)\n",
      "Requirement already satisfied: formulaic>=0.2.2 in c:\\dev\\miniconda3\\lib\\site-packages (from lifelines>=0.25->ngboost) (0.4.0)\n",
      "Requirement already satisfied: autograd>=1.3 in c:\\dev\\miniconda3\\lib\\site-packages (from lifelines>=0.25->ngboost) (1.4)\n",
      "Requirement already satisfied: matplotlib>=3.0 in c:\\dev\\miniconda3\\lib\\site-packages (from lifelines>=0.25->ngboost) (3.5.3)\n",
      "Requirement already satisfied: autograd-gamma>=0.3 in c:\\dev\\miniconda3\\lib\\site-packages (from lifelines>=0.25->ngboost) (0.5.0)\n",
      "Requirement already satisfied: future>=0.15.2 in c:\\dev\\miniconda3\\lib\\site-packages (from autograd>=1.3->lifelines>=0.25->ngboost) (0.18.2)\n",
      "Requirement already satisfied: astor>=0.8 in c:\\dev\\miniconda3\\lib\\site-packages (from formulaic>=0.2.2->lifelines>=0.25->ngboost) (0.8.1)\n",
      "Requirement already satisfied: wrapt>=1.0 in c:\\dev\\miniconda3\\lib\\site-packages (from formulaic>=0.2.2->lifelines>=0.25->ngboost) (1.14.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.2.0 in c:\\dev\\miniconda3\\lib\\site-packages (from formulaic>=0.2.2->lifelines>=0.25->ngboost) (4.3.0)\n",
      "Requirement already satisfied: interface-meta<2.0.0,>=1.2.0 in c:\\dev\\miniconda3\\lib\\site-packages (from formulaic>=0.2.2->lifelines>=0.25->ngboost) (1.3.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\dev\\miniconda3\\lib\\site-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (9.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\dev\\miniconda3\\lib\\site-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\dev\\miniconda3\\lib\\site-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\dev\\miniconda3\\lib\\site-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (3.0.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\dev\\miniconda3\\lib\\site-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\dev\\miniconda3\\lib\\site-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (4.35.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\dev\\miniconda3\\lib\\site-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (21.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\dev\\miniconda3\\lib\\site-packages (from pandas>=1.0.0->lifelines>=0.25->ngboost) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\dev\\miniconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.0->lifelines>=0.25->ngboost) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\dev\\miniconda3\\lib\\site-packages (from scikit-learn>=0.21->ngboost) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\dev\\miniconda3\\lib\\site-packages (from scikit-learn>=0.21->ngboost) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\dev\\miniconda3\\lib\\site-packages (from tqdm>=4.3->ngboost) (0.4.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\dev\\miniconda3\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: scipy in c:\\dev\\miniconda3\\lib\\site-packages (from lightgbm) (1.9.0)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\dev\\miniconda3\\lib\\site-packages (from lightgbm) (1.1.2)\n",
      "Requirement already satisfied: wheel in c:\\dev\\miniconda3\\lib\\site-packages (from lightgbm) (0.37.1)\n",
      "Requirement already satisfied: numpy in c:\\dev\\miniconda3\\lib\\site-packages (from lightgbm) (1.23.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\dev\\miniconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\dev\\miniconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (1.1.0)\n",
      "Requirement already satisfied: hyperopt in c:\\dev\\miniconda3\\lib\\site-packages (0.2.7)\n",
      "Requirement already satisfied: numpy in c:\\dev\\miniconda3\\lib\\site-packages (from hyperopt) (1.23.2)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\dev\\miniconda3\\lib\\site-packages (from hyperopt) (2.8.6)\n",
      "Requirement already satisfied: cloudpickle in c:\\dev\\miniconda3\\lib\\site-packages (from hyperopt) (2.1.0)\n",
      "Requirement already satisfied: py4j in c:\\dev\\miniconda3\\lib\\site-packages (from hyperopt) (0.10.9.7)\n",
      "Requirement already satisfied: six in c:\\dev\\miniconda3\\lib\\site-packages (from hyperopt) (1.16.0)\n",
      "Requirement already satisfied: scipy in c:\\dev\\miniconda3\\lib\\site-packages (from hyperopt) (1.9.0)\n",
      "Requirement already satisfied: tqdm in c:\\dev\\miniconda3\\lib\\site-packages (from hyperopt) (4.63.0)\n",
      "Requirement already satisfied: future in c:\\dev\\miniconda3\\lib\\site-packages (from hyperopt) (0.18.2)\n",
      "Requirement already satisfied: colorama in c:\\dev\\miniconda3\\lib\\site-packages (from tqdm->hyperopt) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "# hyper parameter tuning을 위한 패키지 설치\n",
    "!pip install optuna\n",
    "!pip install catboost\n",
    "!pip install skranger\n",
    "!pip install ngboost\n",
    "!pip install lightgbm\n",
    "!pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pcG2aSVkkAVz"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 41>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minspection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m permutation_importance\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# 모듈화된 함수 사용\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpreprocessing\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstacking\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mstk\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "# 기본 modules\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "# 머신러닝 modules\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from ngboost import NGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import ElasticNet, LinearRegression, Lasso, Ridge\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from skranger.ensemble import RangerForestRegressor\n",
    "from sklearn.neighbors import RadiusNeighborsRegressor\n",
    "from ngboost.scores import LogScore\n",
    "\n",
    "from hyperopt import fmin, hp, tpe\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# 모듈화된 함수 사용\n",
    "import utils.preprocessing as preprocessing\n",
    "import utils.utils as utils\n",
    "import utils.stacking as stk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed 고정\n",
    "def seed_everything(seed): \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    seed = 42\n",
    "    epochs = 200\n",
    "    cv=10\n",
    "    test_size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_Feature별 NRMSE의 총합\n",
    "def lg_nrmse(gt, preds):\n",
    "    \"\"\"\n",
    "    @Description: Metric used in this project\n",
    "    @Params1: gt, pandas dataframe\n",
    "    @Param2: preds, pandas dataframe\n",
    "    @Return: nrmse score\n",
    "    \"\"\"\n",
    "    preds = pd.DataFrame(preds)\n",
    "    all_nrmse = []\n",
    "    for idx in range(0,14):\n",
    "        rmse = mean_squared_error(gt.iloc[:,idx], preds.iloc[:,idx], squared=False)\n",
    "        nrmse = rmse/np.mean(np.abs(gt.iloc[:,idx]))\n",
    "        all_nrmse.append(nrmse)\n",
    "    score = 1.2 * np.sum(all_nrmse[:8]) + 1.0 * np.sum(all_nrmse[8:15]) # Y_01 ~ Y_08 까지 20% 가중치 부여\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_Feature 개별 NRMSE 계산\n",
    "def lg_individual_nrmse(gt, preds):\n",
    "    \"\"\"\n",
    "    @Description: Metric used in this project (individual)\n",
    "    @Params1: gt, pandas dataframe\n",
    "    @Param2: preds, pandas dataframe\n",
    "    @Return: nrmse score\n",
    "    \"\"\"\n",
    "    rmse = mean_squared_error(gt, preds, squared=False)\n",
    "    nrmse = rmse/np.mean(np.abs(gt))\n",
    "    return nrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터에서 X_feature, Y_feature 구분\n",
    "def dataset_split_X_y(df):    \n",
    "    \"\"\"\n",
    "    @Description: split data into features and labels\n",
    "    @Param: df, pandas dataframe with columns starting with X for features and Y for labels\n",
    "    @Return: features and labels in pandas dataframes\n",
    "    \"\"\"\n",
    "    xs = df.filter(regex='X') # Input : X Feature\n",
    "    ys = df.filter(regex='Y') # Output : Y Feature\n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF 안의 결측치(NA)를 확인\n",
    "def check_for_NAs(df, show=False):\n",
    "    \"\"\"\n",
    "    @Description: checks for the NAs in the dataframe\n",
    "    @Param1: df, pandas dataframe\n",
    "    @Param2: show, boolean indicating whether NaN data are also necessary as a part of the output\n",
    "    @Return: name of the columns with NaN\n",
    "    \"\"\"\n",
    "    nan_values = df.loc[:, df.isnull().any()]\n",
    "    if show:\n",
    "        return df[df.isna().any(axis=1)]\n",
    "    return list(nan_values.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF 안의 결측치(NA)를 확인\n",
    "def check_for_NAs(df, show=False):\n",
    "    \"\"\"\n",
    "    @Description: checks for the NAs in the dataframe\n",
    "    @Param1: df, pandas dataframe\n",
    "    @Param2: show, boolean indicating whether NaN data are also necessary as a part of the output\n",
    "    @Return: name of the columns with NaN\n",
    "    \"\"\"\n",
    "    nan_values = df.loc[:, df.isnull().any()]\n",
    "    if show:\n",
    "        return df[df.isna().any(axis=1)]\n",
    "    return list(nan_values.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분산이 0인 feature 탐색\n",
    "def zero_variance(train_x):\n",
    "    \"\"\"\n",
    "    @Description: check for zero_variance\n",
    "    @Param1: df, pandas dataframe\n",
    "    @Return: names of the columns with zero variance\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for col in train_x.columns:\n",
    "        if train_x[col].var() == 0:\n",
    "            result.append(col)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 높은 상관계수값과 feature 탐색\n",
    "def get_top_correlation(df, n=10):\n",
    "    \"\"\"\n",
    "    @Description: print out top correlated features\n",
    "    @Param1: df, pandas dataframe\n",
    "    @Param2: n, number of lines to print \n",
    "    @Return: pandas series\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    for idx1 in range(0, df.shape[1]):\n",
    "        for idx2 in range(0, idx1+1):\n",
    "            pairs.add((df.columns[idx1], df.columns[idx2]))\n",
    "    corr = df.corr().abs().unstack()\n",
    "    corr = corr.drop(labels=pairs).sort_values(ascending=False)\n",
    "    return corr[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화 진행 및 이상치 탐색\n",
    "def find_outlier_zscore(data, threshold = 3):\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    zs = [(y - mean) / std for y in data]\n",
    "    masks = np.where(np.abs(zs) > threshold)\n",
    "    return masks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram 시각화\n",
    "def adjacent_histogram_boxplot(feature_var, figsize = (7, 5)):\n",
    "    \"\"\"\n",
    "    @Description: plot histogram and boxplot in next to each other\n",
    "    @Param1: feature_var, pandas series \n",
    "    @Param2: figsize, size of the figure \n",
    "    \"\"\"\n",
    "    fig, (hist_plot, box_plot) = plt.subplots(nrows=2, sharex=True, gridspec_kw={'height_ratios':(.85,.15)}, figsize=figsize)\n",
    "    sns.distplot(feature_var, kde=True, ax=hist_plot, kde_kws= {\"linewidth\":1.5}) \n",
    "    sns.boxplot(feature_var, ax=box_plot, linewidth = 1, width = 0.5)\n",
    "    hist_plot.set_ylabel('')    \n",
    "    hist_plot.set_xlabel('')\n",
    "    box_plot.set_xlabel('')\n",
    "    hist_plot.tick_params(labelsize=8)\n",
    "    box_plot.tick_params(labelsize=8)\n",
    "    fig.suptitle(feature_var.name, fontsize = 10)\n",
    "    hist_plot.axvline(np.mean(feature_var),color='red',linestyle='-',lw = 1.5)\n",
    "    hist_plot.axvline(np.median(feature_var),color='green',linestyle='--',lw = 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 불러오기\n",
    "def load_data(train, test):\n",
    "    train_df = pd.read_csv(train)\n",
    "    test_df = pd.read_csv(test)\n",
    "\n",
    "    train_x, train_y = dataset_split_X_y(train_df)\n",
    "    cols_with_zero_variance = zero_variance(train_x) # 분산이 0 (통과 여부)\n",
    "    train_x = train_x.drop(cols_with_zero_variance, axis=1)\n",
    "    \n",
    "    test_df = test_df.drop(cols_with_zero_variance, axis=1)\n",
    "\n",
    "    train_x = train_x.drop(['X_10', 'X_11'], axis = 1) # 결측치가 많음 (결측치 = 0, 공지사항)\n",
    "    test_df = test_df.drop(['X_10', 'X_11'], axis = 1)\n",
    "\n",
    "    test_df = test_df.drop('ID', axis=1) \n",
    "\n",
    "    return train_x, train_y, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FB59rtt-llsC"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mutils\u001b[49m\u001b[38;5;241m.\u001b[39mseed_everything(utils\u001b[38;5;241m.\u001b[39mConfig\u001b[38;5;241m.\u001b[39mseed)\n\u001b[0;32m      3\u001b[0m train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData/train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m test_x \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData/test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'utils' is not defined"
     ]
    }
   ],
   "source": [
    "utils.seed_everything(utils.Config.seed)\n",
    "\n",
    "train_df = pd.read_csv('Data/train.csv')\n",
    "test_x = pd.read_csv('Data/test.csv')\n",
    "train_x, train_y = preprocessing.dataset_split_X_y(train_df)\n",
    "\n",
    "cols_with_zero_variance = preprocessing.zero_variance(train_x) # 분산이 0 (통과 여부)\n",
    "train_x = train_x.drop(cols_with_zero_variance, axis = 1)\n",
    "test_x = test_x.drop(cols_with_zero_variance, axis = 1)\n",
    "\n",
    "train_x = train_x.drop(['X_10', 'X_11'], axis = 1) # 결측치가 많음 http://localhost:8888/notebooks/20220801%20LG%20AI%20Research%20%EC%9E%90%EC%9C%A8%EC%A3%BC%ED%96%89%20%EC%84%BC%EC%84%9C%EC%9D%98%20%EC%95%88%ED%85%8C%EB%82%98%20%EC%84%B1%EB%8A%A5%20%EC%98%88%EC%B8%A1%20AI%20%EA%B2%BD%EC%A7%84%EB%8C%80%ED%9A%8C/Model_Submit/Submit1_Stacking%20ensemble_Full_Version.ipynb#(결측치 = 0, 공지사항)\n",
    "test_x = test_x.drop(['X_10', 'X_11'], axis = 1)\n",
    "\n",
    "test_x = test_x.drop('ID', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "- 모델별 개별학습(타겟 Y_01~Y_14) 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Setting\n",
    "def lgbm_objective(params):\n",
    "    params = {\n",
    "        'n_estimators': int(params['n_estimators']),\n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'num_leaves': int(params['num_leaves']),\n",
    "        'min_child_samples': int(params['min_child_samples']),\n",
    "        'colsample_bytree': '{:.5f}'.format(params['colsample_bytree']),\n",
    "        'subsample': '{:.5f}'.format(params['subsample']),\n",
    "        'min_split_gain': '{:.5f}'.format(params['min_split_gain']),\n",
    "        'scale_pos_weight': '{:.5f}'.format(params['scale_pos_weight']),\n",
    "        'reg_alpha': '{:.5f}'.format(params['reg_alpha']),\n",
    "        'reg_lambda': '{:.5f}'.format(params['reg_lambda']),\n",
    "        'learning_rate': '{:.5f}'.format(params['learning_rate']),   \n",
    "    }\n",
    "\n",
    "    model = LGBMRegressor(\n",
    "        n_jobs = -1,\n",
    "        random_state = 1,\n",
    "        verbose = 100,\n",
    "        **params\n",
    "    )\n",
    "\n",
    "    losses = np.sqrt(-cross_val_score(model, train_x, train_y['Y_01'], cv=Config.cv, scoring='neg_mean_squared_error')) # cross_val_score : 교차검증\n",
    "    losses = losses / np.mean(np.abs(train_y['Y_01']))\n",
    "    print(\"NRMSE Loss {:.5f} params {}\".format(losses.mean(), params))\n",
    "    return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Tunning\n",
    "space_lgbm = {\n",
    "    'n_estimators' : hp.quniform('n_estimators', 100, 1500, 1),\n",
    "    'max_depth': hp.quniform('max_depth', 5, 250, 1),\n",
    "    'num_leaves': hp.quniform('num_leaves', 20, 200, 5),\n",
    "    'min_child_samples': hp.quniform('min_child_samples', 10, 150, 5),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1.0),\n",
    "    'subsample': hp.uniform('subsample', 0.3, 1.0),\n",
    "    'min_split_gain': hp.uniform('min_split_gain', 0, 0.7),\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 1, 10),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 500),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0, 500),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.5)),\n",
    "}\n",
    "\n",
    "best = fmin(fn = lgbm_objective,\n",
    "            space = space_lgbm,\n",
    "            algo = tpe.suggest,\n",
    "            verbose = 10,\n",
    "            max_evals = 200)\n",
    "\n",
    "print(best)\n",
    "best['n_estimators'] = int(best['n_estimators'])\n",
    "best['num_leaves'] = int(best['num_leaves'])\n",
    "best['max_depth'] = int(best['max_depth'])\n",
    "best['min_child_samples'] = int(best['min_child_samples'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Setting\n",
    "def cat_objective(params):\n",
    "    params = {\n",
    "        'n_estimators': int(params['n_estimators']),\n",
    "        'depth': int(params['depth']),\n",
    "        'learning_rate': params['learning_rate'],   \n",
    "        'l2_leaf_reg': params['l2_leaf_reg'],\n",
    "        'max_bin': int(params['max_bin']),\n",
    "        'min_data_in_leaf': int(params['min_data_in_leaf']),\n",
    "        'random_strength': params['random_strength'],\n",
    "        'fold_len_multiplier': params['fold_len_multiplier'],\n",
    "        \n",
    "    }\n",
    "\n",
    "    model = CatBoostRegressor(\n",
    "        logging_level='Silent',\n",
    "        **params\n",
    "    )\n",
    "\n",
    "    losses = np.sqrt(-cross_val_score(model, train_x, train_y['Y_01'], cv=Config.cv, scoring='neg_mean_squared_error'))\n",
    "    losses = losses / np.mean(np.abs(train_y['Y_01']))\n",
    "    print(\"NRMSE Loss {:.5f} params {}\".format(losses.mean(), params))\n",
    "    return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Tunning\n",
    "space_catboost = {\n",
    "    'n_estimators' : hp.quniform('n_estimators', 100, 300, 50),\n",
    "    'depth': hp.quniform(\"depth\", 2, 16, 1),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n",
    "    'l2_leaf_reg': hp.uniform('l2_leaf_reg', 3, 8),\n",
    "    'max_bin' : hp.quniform('max_bin', 1, 254, 1),\n",
    "    'min_data_in_leaf' : hp.quniform('min_data_in_leaf', 2, 700, 1),\n",
    "    'random_strength' : hp.loguniform('random_strength', np.log(0.005), np.log(5)),\n",
    "    'fold_len_multiplier' : hp.loguniform('fold_len_multiplier', np.log(1.01), np.log(2.5)),\n",
    "}\n",
    "\n",
    "best = fmin(fn = cat_objective,\n",
    "            space = space_catboost,\n",
    "            algo = tpe.suggest,\n",
    "            verbose = 1,\n",
    "            max_evals = 200)\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Setting\n",
    "def extra_objective(params):\n",
    "    params = {\n",
    "        'n_estimators': int(params['n_estimators']),\n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'min_samples_split': int(params['min_samples_split']),\n",
    "        'min_samples_leaf': int(params['min_samples_leaf']),\n",
    "        'min_weight_fraction_leaf': params['min_weight_fraction_leaf'],\n",
    "        'max_features': params['max_features'],\n",
    "        'max_leaf_nodes': int(params['max_leaf_nodes']),\n",
    "        'min_impurity_decrease': params['min_impurity_decrease'],\n",
    "        'bootstrap': params['bootstrap'],\n",
    "        'ccp_alpha': params['ccp_alpha'],  \n",
    "    }\n",
    "\n",
    "    model = ExtraTreesRegressor(\n",
    "        n_jobs = -1,\n",
    "        verbose = 0,\n",
    "        random_state = 1,\n",
    "        **params\n",
    "    )\n",
    "\n",
    "    losses = np.sqrt(-cross_val_score(model, train_x, train_y['Y_01'], cv=Config.cv, scoring='neg_mean_squared_error'))\n",
    "    losses = losses / np.mean(np.abs(train_y['Y_01']))\n",
    "    print(\"NRMSE Loss {:.5f} params {}\".format(losses.mean(), params))\n",
    "    return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Tunning\n",
    "space_extra = {\n",
    "    'n_estimators' : hp.quniform('n_estimators', 100, 1500, 50),\n",
    "    'max_depth': hp.quniform('max_depth', 3, 50, 1),\n",
    "    'min_samples_split': hp.quniform('min_samples_split', 5, 50, 5),\n",
    "    'min_samples_leaf': hp.quniform('min_samples_leaf', 5, 50, 1),\n",
    "    'min_weight_fraction_leaf': hp.uniform('min_weight_fraction_leaf', 0.01, 0.5),\n",
    "    'max_features': hp.choice('max_features', ['sqrt', 'log2', None, 'auto']),\n",
    "    'max_leaf_nodes': hp.quniform('max_leaf_nodes', 3, 30, 1),\n",
    "    'min_impurity_decrease': hp.uniform('min_impurity_decrease', 0, 200),\n",
    "    'bootstrap':  hp.choice('bootstrap', [True, False]),\n",
    "    'ccp_alpha': hp.uniform('ccp_alpha', 0.01, 1.0),\n",
    "}\n",
    "\n",
    "best = fmin(fn = extra_objective,\n",
    "            space = space_extra,\n",
    "            algo = tpe.suggest,\n",
    "            verbose = 1,\n",
    "            max_evals = 2)\n",
    "\n",
    "best['n_estimators'] = int(best['n_estimators'])\n",
    "best['max_depth'] = int(best['max_depth'])\n",
    "best['max_leaf_nodes'] = int(best['max_leaf_nodes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NGBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Setting\n",
    "def ngbr_objective(params):\n",
    "    params = {\n",
    "        'n_estimators': int(params['n_estimators']),\n",
    "        'learning_rate': params['learning_rate'],\n",
    "        'natural_gradient': params['natural_gradient'],\n",
    "        'col_sample': float(params['col_sample']),\n",
    "        'minibatch_frac': float(params['minibatch_frac']),\n",
    "        'tol': float(params['tol']),\n",
    "    }\n",
    "\n",
    "    model = NGBRegressor(\n",
    "        verbose = 100,\n",
    "        random_state = 1,\n",
    "        **params\n",
    "    )\n",
    "\n",
    "    losses = np.sqrt(-cross_val_score(model, train_x, train_y['Y_01'], cv=Config.cv, scoring='neg_mean_squared_error'))\n",
    "    losses = losses / np.mean(np.abs(train_y['Y_01']))\n",
    "    print(\"NRMSE Loss {:.5f} params {}\".format(losses.mean(), params))\n",
    "    return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Tunning\n",
    "space_ngboost = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 100, 500, 10),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n",
    "    'natural_gradient': hp.choice('natural_gradient', [True, False]),\n",
    "    'col_sample': hp.quniform('col_sample', 0, 1, 0.01),\n",
    "    'minibatch_frac': hp.quniform('minibatch_frac', 0, 1, 0.01),\n",
    "    'tol': hp.uniform('tol', 1e-6, 3e-4),\n",
    "}\n",
    "\n",
    "best = fmin(fn = ngbr_objective,\n",
    "            space = space_ngboost,\n",
    "            algo = tpe.suggest,\n",
    "            verbose = 10,\n",
    "            max_evals = 100)\n",
    "\n",
    "print(best)\n",
    "best['n_estimators'] = int(best['n_estimators'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Ensenble\n",
    "- 모델별 개별학습(타겟 Y_01~Y_14) 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stacking_base_datasets(model, train_x, train_y, col,test, params):\n",
    "    kf = KFold(n_splits=10, shuffle=False)\n",
    "    train_fold_pred = np.zeros((train_x.shape[0],1))\n",
    "    test_pred = np.zeros((test.shape[0],10))\n",
    "    \n",
    "    \n",
    "    for folder_counter, (train_index, valid_index) in enumerate(kf.split(train_x)):\n",
    "        print('Fold : ', folder_counter, ' Start')\n",
    "        X_tr = train_x.loc[train_index]\n",
    "        y_tr = train_y[col].loc[train_index]\n",
    "        X_te = train_x.loc[valid_index] \n",
    "        \n",
    "        if model == 'cat':\n",
    "          model = CatBoostRegressor(random_state=1,\n",
    "                                    **params)\n",
    "        \n",
    "        elif model == 'extra':\n",
    "          model = ExtraTreesRegressor(random_state=1, \n",
    "                                      **params)\n",
    "\n",
    "        elif model == 'ngbr':\n",
    "          model = NGBRegressor(random_state = 1)\n",
    "        \n",
    "        elif model == 'lgbm':\n",
    "          model = LGBMRegressor(random_state=1, n_jobs=-1, \n",
    "                                **params)\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        train_fold_pred[valid_index, :] = model.predict(X_te).reshape(-1,1) \n",
    "        test_pred[:, folder_counter] = model.predict(test) \n",
    "        \n",
    "    test_pred_mean = np.mean(test_pred, axis=1).reshape(-1,1)\n",
    "    \n",
    "    return train_fold_pred, test_pred_mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-PxkCnAsG6Q"
   },
   "outputs": [],
   "source": [
    "# Y_01\n",
    "#min_samples_split\n",
    "\n",
    "cat_01 = {'depth': 9, 'fold_len_multiplier': 1.6722688563924544, 'l2_leaf_reg': 9.992348977307927, 'learning_rate': 0.03686783566671033, 'max_bin': 16, 'min_data_in_leaf': 7, 'n_estimators': 500, 'random_strength': 0.5478002316160607}\n",
    "\n",
    "extra_01 = {'bootstrap': 0, 'ccp_alpha': 0.5956203348598316, 'max_depth': 43, 'max_features': 1, 'max_leaf_nodes': 22, 'min_impurity_decrease': 195.59697998782488, 'min_samples_leaf': 0.4, 'min_samples_split': 0.427, 'min_weight_fraction_leaf': 0.3872874854064327, 'n_estimators': 200}\n",
    "\n",
    "lgbm_01 = {'colsample_bytree': 0.572280100273023, 'learning_rate': 0.010283635038627429, 'max_depth': 180, 'min_child_samples': 135, 'min_split_gain': 0.04511227284338413, 'n_estimators': 900, 'num_leaves': 70, 'reg_alpha': 4.406681827912319, 'reg_lambda': 20.4785600448913, 'scale_pos_weight': 8.302374117433086, 'subsample': 0.1688669888026464}\n",
    "\n",
    "ngbr_01 = {'n_estimators': 250, 'learning_rate': 0.027115337704182965, 'natural_gradient': True, 'col_sample': 0.2, 'minibatch_frac': 0.8, 'tol': 5.5136412071576055e-05}\n",
    "\n",
    "xx_train, xx_test = stk.get_stacking_base_datasets('cat', train_x, train_y, col='Y_01', test=test_x, params = cat_01)\n",
    "yy_train, yy_test = stk.get_stacking_base_datasets('extra', train_x, train_y, col='Y_01', test=test_x, params = extra_01)\n",
    "zz_train, zz_test = stk.get_stacking_base_datasets('lgbm', train_x, train_y, col='Y_01', test=test_x, params = lgbm_01)\n",
    "qq_train, qq_test = stk.get_stacking_base_datasets('ngbr', train_x, train_y, col='Y_01', test=test_x, params = ngbr_01)\n",
    "\n",
    "\n",
    "Stack_final_X_train = np.concatenate((xx_train, yy_train, zz_train, qq_train), axis=1)\n",
    "Stack_final_X_test = np.concatenate((xx_test, yy_test, zz_test, qq_test), axis=1)\n",
    "\n",
    "# final_model 선택\n",
    "lr_final = LGBMRegressor(**lgbm_01)\n",
    "lr_final.fit(Stack_final_X_train, train_y['Y_01'])\n",
    "stack_final1 = lr_final.predict(Stack_final_X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Y_02\n",
    "#min_samples_split\n",
    "\n",
    "cat_02 = {'depth': 9, 'fold_len_multiplier': 1.8379420467251593, 'l2_leaf_reg': 27.102344731579784, 'learning_rate': 0.03597820559455176, 'max_bin': 14, 'min_data_in_leaf': 2, 'n_estimators': 500, 'random_strength': 0.9800368067026318}\n",
    "\n",
    "extra_02 = {'bootstrap': 0, 'ccp_alpha': 0.9020983921597531, 'max_depth': 42, 'max_features': 2, 'max_leaf_nodes': 7, 'min_impurity_decrease': 150.7925115966371, 'min_samples_leaf': 18, 'min_samples_split': 15, 'min_weight_fraction_leaf': 0.3321219768527379, 'n_estimators': 200}\n",
    "\n",
    "lgbm_02 =  {'colsample_bytree': 0.7641322280477741, 'learning_rate': 0.010977205425053654, 'max_depth': 90, 'min_child_samples': 75, 'min_split_gain': 0.13379952895779884, 'n_estimators': 900, 'num_leaves': 80, 'reg_alpha': 1.9214119194170154, 'reg_lambda': 14.454450236504218, 'scale_pos_weight': 2.171961031806387, 'subsample': 0.9552593593877317}\n",
    "\n",
    "ngbr_02 = {'n_estimators': 250, 'learning_rate': 0.0656349966273891, 'natural_gradient': True, 'col_sample': 0.8, 'minibatch_frac': 0.55, 'tol': 0.00028029350235701545}\n",
    "\n",
    "xx_train, xx_test = stk.get_stacking_base_datasets('cat', train_x, train_y, col='Y_02', test=test_x, params = cat_02)\n",
    "yy_train, yy_test = stk.get_stacking_base_datasets('extra', train_x, train_y, col='Y_02', test=test_x, params = extra_02)\n",
    "zz_train, zz_test = stk.get_stacking_base_datasets('lgbm', train_x, train_y, col='Y_02', test=test_x, params = lgbm_02)\n",
    "qq_train, qq_test = stk.get_stacking_base_datasets('ngbr', train_x, train_y, col='Y_02', test=test_x, params = ngbr_02)\n",
    "\n",
    "\n",
    "Stack_final_X_train = np.concatenate((xx_train, yy_train, zz_train, qq_train), axis=1)\n",
    "Stack_final_X_test = np.concatenate((xx_test, yy_test, zz_test, qq_test), axis=1)\n",
    "\n",
    "# final_model 선택\n",
    "lr_final = LGBMRegressor(**lgbm_02)\n",
    "lr_final.fit(Stack_final_X_train, train_y['Y_02'])\n",
    "stack_final2 = lr_final.predict(Stack_final_X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Y_03\n",
    "#min_samples_split\n",
    "\n",
    "cat_03 = {'depth': 7, 'fold_len_multiplier': 2.498263900973405, 'l2_leaf_reg': 36.312606304859514, 'learning_rate': 0.043147429358500425, 'max_bin': 14, 'min_data_in_leaf': 2, 'n_estimators': 450, 'random_strength': 0.6650471273750369}\n",
    "\n",
    "extra_03 = {'bootstrap': 1, 'ccp_alpha': 0.9068334703113171, 'max_depth': 22, 'max_features': 2, 'max_leaf_nodes': 11, 'min_impurity_decrease': 6.407983868655598, 'min_samples_leaf': 17, 'min_samples_split': 10, 'min_weight_fraction_leaf': 0.4864839919729328, 'n_estimators': 1450}\n",
    "\n",
    "lgbm_03 = {'colsample_bytree': 0.5504769098255781,  'learning_rate': 0.019653385015120244, 'max_depth': 220, 'min_child_samples': 25, 'min_split_gain': 0.1273611040963466, 'n_estimators': 470, 'num_leaves': 160, 'reg_alpha': 3.5549669150756706, 'reg_lambda': 39.88636182674132, 'scale_pos_weight': 12.46696320152359, 'subsample': 0.7590007450921917}\n",
    "\n",
    "ngbr_03 = {'n_estimators': 410, 'learning_rate': 0.11407060690033853, 'natural_gradient': True, 'col_sample': 0.25, 'minibatch_frac': 1, 'tol': 0.000166350313681024}\n",
    "\n",
    "xx_train, xx_test = stk.get_stacking_base_datasets('cat', train_x, train_y, col='Y_03', test=test_x, params = cat_03)\n",
    "yy_train, yy_test = stk.get_stacking_base_datasets('extra', train_x, train_y, col='Y_03', test=test_x, params = extra_03)\n",
    "zz_train, zz_test = stk.get_stacking_base_datasets('lgbm', train_x, train_y, col='Y_03', test=test_x, params = lgbm_03)\n",
    "qq_train, qq_test = stk.get_stacking_base_datasets('ngbr', train_x, train_y, col='Y_03', test=test_x, params = ngbr_03)\n",
    "\n",
    "\n",
    "Stack_final_X_train = np.concatenate((xx_train, yy_train, zz_train, qq_train), axis=1)\n",
    "Stack_final_X_test = np.concatenate((xx_test, yy_test, zz_test, qq_test), axis=1)\n",
    "\n",
    "# final_model 선택\n",
    "lr_final = LGBMRegressor(**lgbm_03)\n",
    "lr_final.fit(Stack_final_X_train, train_y['Y_03'])\n",
    "stack_final3 = lr_final.predict(Stack_final_X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Y_04\n",
    "#min_samples_split\n",
    "\n",
    "cat_04 = {'depth': 10, 'fold_len_multiplier': 1.3980250309157025, 'l2_leaf_reg': 5.838850864558845, 'learning_rate': 0.2737350830778467, 'max_bin': 78, 'min_data_in_leaf': 561, 'n_estimators': 20, 'random_strength': 0.009672204431612739}\n",
    "\n",
    "extra_04 = {'bootstrap': 0, 'ccp_alpha': 0.7076108565007323, 'max_depth': 48, 'max_features': 3, 'max_leaf_nodes': 16, 'min_impurity_decrease': 45.084212905659186, 'min_samples_leaf': 11, 'min_samples_split': 20, 'min_weight_fraction_leaf': 0.1164511175367393, 'n_estimators': 100}\n",
    "\n",
    "lgbm_04 = {'colsample_bytree': 0.5597537952569402, 'learning_rate': 0.02374663979814546, 'max_depth': 32, 'min_child_samples': 100, 'min_split_gain': 0.12211426885216736, 'n_estimators': 1263, 'num_leaves': 200, 'reg_alpha': 14.606693962963451, 'reg_lambda': 299.52278825209424, 'scale_pos_weight': 7.7785016838070735, 'subsample': 0.6254745287838821}\n",
    "\n",
    "ngbr_04 = {'n_estimators': 431, 'learning_rate': 0.08883519586581468, 'natural_gradient': True, 'col_sample': 0.97, 'minibatch_frac': 0.75, 'tol': 8.896682623929568e-05}\n",
    "\n",
    "xx_train, xx_test = stk.get_stacking_base_datasets('cat', train_x, train_y, col='Y_04', test=test_x, params = cat_04)\n",
    "yy_train, yy_test = stk.get_stacking_base_datasets('extra', train_x, train_y, col='Y_04', test=test_x, params = extra_04)\n",
    "zz_train, zz_test = stk.get_stacking_base_datasets('lgbm', train_x, train_y, col='Y_04', test=test_x, params = lgbm_04)\n",
    "qq_train, qq_test = stk.get_stacking_base_datasets('ngbr', train_x, train_y, col='Y_04', test=test_x, params = ngbr_04)\n",
    "\n",
    "\n",
    "Stack_final_X_train = np.concatenate((xx_train, yy_train, zz_train, qq_train), axis=1)\n",
    "Stack_final_X_test = np.concatenate((xx_test, yy_test, zz_test, qq_test), axis=1)\n",
    "\n",
    "# final_model 선택\n",
    "lr_final = LGBMRegressor(**lgbm_04)\n",
    "lr_final.fit(Stack_final_X_train, train_y['Y_04'])\n",
    "stack_final4 = lr_final.predict(Stack_final_X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Y_05\n",
    "#min_samples_split\n",
    "\n",
    "cat_05 = {'depth': 9, 'fold_len_multiplier': 1.7527609013156893, 'l2_leaf_reg': 5.150371128645829, 'learning_rate': 0.23166991521375363, 'max_bin': 181, 'min_data_in_leaf': 591, 'n_estimators': 17, 'random_strength': 0.08626442162325075}\n",
    "\n",
    "extra_05 = {'bootstrap': 0, 'ccp_alpha': 0.17223432236304015, 'max_depth': 39, 'max_features': 2, 'max_leaf_nodes': 16, 'min_impurity_decrease': 166.70077338146032, 'min_samples_leaf': 34, 'min_samples_split': 15, 'min_weight_fraction_leaf': 0.22146453407955657, 'n_estimators': 200}\n",
    "\n",
    "lgbm_05 = {'colsample_bytree': 0.4311015575880258, 'learning_rate': 0.01749725932551278, 'max_depth': 53, 'min_child_samples': 15, 'min_split_gain': 0.2820951740673634, 'n_estimators': 974, 'num_leaves': 165, 'reg_alpha': 9.604623064885754, 'reg_lambda': 12.314490508636432, 'scale_pos_weight': 6.6422956907936825, 'subsample': 0.7390190399971659}\n",
    "\n",
    "ngbr_05 = {'n_estimators': 449, 'learning_rate': 0.08850472848590257, 'natural_gradient': True, 'col_sample': 0.8, 'minibatch_frac': 0.97, 'tol': 7.048508838263751e-05}\n",
    "\n",
    "xx_train, xx_test = stk.get_stacking_base_datasets('cat', train_x, train_y, col='Y_05', test=test_x, params = cat_05)\n",
    "yy_train, yy_test = stk.get_stacking_base_datasets('extra', train_x, train_y, col='Y_05', test=test_x, params = extra_05)\n",
    "zz_train, zz_test = stk.get_stacking_base_datasets('lgbm', train_x, train_y, col='Y_05', test=test_x, params = lgbm_05)\n",
    "qq_train, qq_test = stk.get_stacking_base_datasets('ngbr', train_x, train_y, col='Y_05', test=test_x, params = ngbr_05)\n",
    "\n",
    "\n",
    "Stack_final_X_train = np.concatenate((xx_train, yy_train, zz_train, qq_train), axis=1)\n",
    "Stack_final_X_test = np.concatenate((xx_test, yy_test, zz_test, qq_test), axis=1)\n",
    "\n",
    "# final_model 선택\n",
    "lr_final = LGBMRegressor(**lgbm_05)\n",
    "lr_final.fit(Stack_final_X_train, train_y['Y_05'])\n",
    "stack_final5 = lr_final.predict(Stack_final_X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Y_06\n",
    "#min_samples_split\n",
    "\n",
    "cat_06 = {'depth': 8, 'fold_len_multiplier': 1.4734463589684192, 'l2_leaf_reg': 7.343561976614034, 'learning_rate': 0.2546701358021111, 'max_bin': 8, 'min_data_in_leaf': 280, 'n_estimators': 13, 'random_strength': 0.06423202371274109}\n",
    "\n",
    "extra_06 = {'bootstrap': 0, 'ccp_alpha': 0.263122350467869, 'max_depth': 12, 'max_features': 1, 'max_leaf_nodes': 6, 'min_impurity_decrease': 140.20905071348278, 'min_samples_leaf': 50, 'min_samples_split': 50, 'min_weight_fraction_leaf': 0.17986464144348094, 'n_estimators': 450}\n",
    "\n",
    "lgbm_06 = {'colsample_bytree': 0.6889745043181079, 'learning_rate': 0.06146161938790444, 'max_depth': 89, 'min_child_samples': 10, 'min_split_gain': 0.669592868575692, 'n_estimators': 1169, 'num_leaves': 175, 'reg_alpha': 11.405277636150856, 'reg_lambda': 112.37954230084294, 'scale_pos_weight': 5.932435783263877, 'subsample': 0.8265223228903998}\n",
    "\n",
    "ngbr_06 = {'n_estimators': 153, 'learning_rate': 0.05121743231435252, 'natural_gradient': True, 'col_sample': 0.7000000000000001, 'minibatch_frac': 0.97, 'tol': 0.00013297533787653073}\n",
    "\n",
    "xx_train, xx_test = stk.get_stacking_base_datasets('cat', train_x, train_y, col='Y_06', test=test_x, params = cat_06)\n",
    "yy_train, yy_test = stk.get_stacking_base_datasets('extra', train_x, train_y, col='Y_06', test=test_x, params = extra_06)\n",
    "zz_train, zz_test = stk.get_stacking_base_datasets('lgbm', train_x, train_y, col='Y_06', test=test_x, params = lgbm_06)\n",
    "qq_train, qq_test = stk.get_stacking_base_datasets('ngbr', train_x, train_y, col='Y_06', test=test_x, params = ngbr_06)\n",
    "\n",
    "\n",
    "Stack_final_X_train = np.concatenate((xx_train, yy_train, zz_train, qq_train), axis=1)\n",
    "Stack_final_X_test = np.concatenate((xx_test, yy_test, zz_test, qq_test), axis=1)\n",
    "\n",
    "# final_model 선택\n",
    "lr_final = LGBMRegressor(**lgbm_06)\n",
    "lr_final.fit(Stack_final_X_train, train_y['Y_06'])\n",
    "stack_final6 = lr_final.predict(Stack_final_X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Y_07\n",
    "#min_samples_split\n",
    "\n",
    "cat_07 = {'n_estimators': 350, 'depth': 9, 'learning_rate': 0.07700980062937977, 'l2_leaf_reg': 7.366594895982364, 'max_bin': 80, 'min_data_in_leaf': 654, 'random_strength': 0.12106590929604114, 'fold_len_multiplier': 2.3574644740356874}\n",
    "\n",
    "extra_07 = {'n_estimators': 100, 'max_depth': 9, 'min_samples_split': 15, 'min_samples_leaf': 7, 'min_weight_fraction_leaf': 0.15163758513069608, 'max_features': 'auto', 'max_leaf_nodes': 16, 'min_impurity_decrease': 28.474990166012663, 'bootstrap': True, 'ccp_alpha': 0.8351211481986806}\n",
    "\n",
    "lgbm_07 = {'colsample_bytree': 0.8663251864650988, 'learning_rate': 0.018110306887688978, 'max_depth': 166, 'min_child_samples': 50, 'min_split_gain': 0.025403061552667243, 'n_estimators': 1080, 'num_leaves': 100, 'reg_alpha': 2.0131018839563666, 'reg_lambda': 63.56640846106552, 'scale_pos_weight': 1.8584564419776715, 'subsample': 0.7643028435523616}\n",
    "\n",
    "ngbr_07 = {'n_estimators': 280, 'learning_rate': 0.0543058099317307, 'natural_gradient': False, 'col_sample': 0.61, 'minibatch_frac': 0.53, 'tol': 4.259736507913848e-06}\n",
    "\n",
    "xx_train, xx_test = stk.get_stacking_base_datasets('cat', train_x, train_y, col='Y_07', test=test_x, params = cat_07)\n",
    "yy_train, yy_test = stk.get_stacking_base_datasets('extra', train_x, train_y, col='Y_07', test=test_x, params = extra_07)\n",
    "zz_train, zz_test = stk.get_stacking_base_datasets('lgbm', train_x, train_y, col='Y_07', test=test_x, params = lgbm_07)\n",
    "qq_train, qq_test = stk.get_stacking_base_datasets('ngbr', train_x, train_y, col='Y_07', test=test_x, params = ngbr_07)\n",
    "\n",
    "\n",
    "Stack_final_X_train = np.concatenate((xx_train, yy_train, zz_train, qq_train), axis=1)\n",
    "Stack_final_X_test = np.concatenate((xx_test, yy_test, zz_test, qq_test), axis=1)\n",
    "\n",
    "# final_model 선택\n",
    "lr_final = LGBMRegressor(**lgbm_07)\n",
    "lr_final.fit(Stack_final_X_train, train_y['Y_07'])\n",
    "stack_final7 = lr_final.predict(Stack_final_X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Y_08\n",
    "#min_samples_split\n",
    "\n",
    "cat_08 = {'n_estimators': 450, 'depth': 11, 'learning_rate': 0.06098987016322603, 'l2_leaf_reg': 7.78479685048665, 'max_bin': 244, 'min_data_in_leaf': 423, 'random_strength': 0.4646435148235979, 'fold_len_multiplier': 1.7857138740606202}\n",
    "\n",
    "extra_08 = {'n_estimators': 150, 'max_depth': 31, 'min_samples_split': 45, 'min_samples_leaf': 38, 'min_weight_fraction_leaf': 0.3764820550107259, 'max_features': 'log2', 'max_leaf_nodes': 21, 'min_impurity_decrease': 41.54716726233874, 'bootstrap': True, 'ccp_alpha': 0.9985147070627858}\n",
    "\n",
    "lgbm_08 = {'colsample_bytree': 0.8970390757241629, 'learning_rate': 0.03571726260659087, 'max_depth': 164, 'min_child_samples': 30, 'min_split_gain': 0.2863362850926679, 'n_estimators': 740, 'num_leaves': 100, 'reg_alpha': 1.1167159754886287, 'reg_lambda': 280.9798636389436, 'scale_pos_weight': 4.75867892931176, 'subsample': 0.681716202670263}\n",
    "\n",
    "ngbr_08 = {'n_estimators': 490, 'learning_rate': 0.04770929499260395, 'natural_gradient': True, 'col_sample': 1, 'minibatch_frac': 0.6, 'tol': 0.00019175990157775972}\n",
    "\n",
    "xx_train, xx_test = stk.get_stacking_base_datasets('cat', train_x, train_y, col='Y_08', test=test_x, params = cat_08)\n",
    "yy_train, yy_test = stk.get_stacking_base_datasets('extra', train_x, train_y, col='Y_08', test=test_x, params = extra_08)\n",
    "zz_train, zz_test = stk.get_stacking_base_datasets('lgbm', train_x, train_y, col='Y_08', test=test_x, params = lgbm_08)\n",
    "qq_train, qq_test = stk.get_stacking_base_datasets('ngbr', train_x, train_y, col='Y_08', test=test_x, params = ngbr_08)\n",
    "\n",
    "\n",
    "Stack_final_X_train = np.concatenate((xx_train, yy_train, zz_train, qq_train), axis=1)\n",
    "Stack_final_X_test = np.concatenate((xx_test, yy_test, zz_test, qq_test), axis=1)\n",
    "\n",
    "# final_model 선택\n",
    "lr_final = LGBMRegressor(**lgbm_08)\n",
    "lr_final.fit(Stack_final_X_train, train_y['Y_08'])\n",
    "stack_final8 = lr_final.predict(Stack_final_X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Y_09\n",
    "#min_samples_split\n",
    "\n",
    "cat_09 = {'n_estimators': 450, 'depth': 7, 'learning_rate': 0.08417441728687995, 'l2_leaf_reg': 7.003716950710109, 'max_bin': 249, 'min_data_in_leaf': 85, 'random_strength': 0.01346018468768982, 'fold_len_multiplier': 1.8510933222341048}\n",
    "\n",
    "extra_09 = {'n_estimators': 50, 'max_depth': 35, 'min_samples_split': 45, 'min_samples_leaf': 11, 'min_weight_fraction_leaf': 0.1446169507139342, 'max_features': None, 'max_leaf_nodes': 10, 'min_impurity_decrease': 190.68425357881898, 'bootstrap': True, 'ccp_alpha': 0.8503455224547213}\n",
    "\n",
    "lgbm_09 = {'n_estimators': 900, 'max_depth': 86, 'num_leaves': 150, 'min_child_samples': 85, 'colsample_bytree': '0.90507', 'subsample': '0.62362', 'min_split_gain': '0.21034', 'scale_pos_weight': '8.77311', 'reg_alpha': '0.07069', 'reg_lambda': '499.10672', 'learning_rate': '0.04679'}\n",
    "\n",
    "ngbr_09 = {'n_estimators': 250, 'learning_rate': 0.0531679007741611, 'natural_gradient': False, 'col_sample': 0.8200000000000001, 'minibatch_frac': 0.65, 'tol': 0.00013547208288125422}\n",
    "\n",
    "xx_train, xx_test = stk.get_stacking_base_datasets('cat', train_x, train_y, col='Y_09', test=test_x, params = cat_09)\n",
    "yy_train, yy_test = stk.get_stacking_base_datasets('extra', train_x, train_y, col='Y_09', test=test_x, params = extra_09)\n",
    "zz_train, zz_test = stk.get_stacking_base_datasets('lgbm', train_x, train_y, col='Y_09', test=test_x, params = lgbm_09)\n",
    "qq_train, qq_test = stk.get_stacking_base_datasets('ngbr', train_x, train_y, col='Y_09', test=test_x, params = ngbr_09)\n",
    "\n",
    "\n",
    "Stack_final_X_train = np.concatenate((xx_train, yy_train, zz_train, qq_train), axis=1)\n",
    "Stack_final_X_test = np.concatenate((xx_test, yy_test, zz_test, qq_test), axis=1)\n",
    "\n",
    "# final_model 선택\n",
    "lr_final = LGBMRegressor(**lgbm_09)\n",
    "lr_final.fit(Stack_final_X_train, train_y['Y_09'])\n",
    "stack_final9 = lr_final.predict(Stack_final_X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Y_10\n",
    "#min_samples_split\n",
    "\n",
    "cat_10 = {'depth': 10, 'fold_len_multiplier': 1.8964686014236263, 'l2_leaf_reg': 6.890506058346803, 'learning_rate': 0.04431783808011194, 'max_bin': 135, 'min_data_in_leaf': 449, 'n_estimators': 400, 'random_strength': 0.0715293516760773}\n",
    "\n",
    "extra_10 = {'bootstrap': 0, 'ccp_alpha': 0.06816154953537701, 'max_depth': 34, 'max_features': 2, 'max_leaf_nodes': 17, 'min_impurity_decrease': 10.897433724670414, 'min_samples_leaf': 45, 'min_samples_split': 25, 'min_weight_fraction_leaf': 0.22786627091058692, 'n_estimators': 100}\n",
    "\n",
    "lgbm_10 = {'colsample_bytree': 0.8350973419202665, 'learning_rate': 0.03134966396365972, 'max_depth': 114, 'min_child_samples': 20, 'min_split_gain': 0.24406788869557822, 'n_estimators': 454, 'num_leaves': 115, 'reg_alpha': 1.0870546166564243, 'reg_lambda': 346.21163772786895, 'scale_pos_weight': 5.81617865285278, 'subsample': 0.45612075761336973}\n",
    "\n",
    "ngbr_10 = {'n_estimators': 400, 'learning_rate': 0.10343505554950799, 'natural_gradient': True, 'col_sample': 0.71, 'minibatch_frac': 0.96, 'tol': 6.075001314642106e-05}\n",
    "\n",
    "xx_train, xx_test = stk.get_stacking_base_datasets('cat', train_x, train_y, col='Y_10', test=test_x, params = cat_10)\n",
    "yy_train, yy_test = stk.get_stacking_base_datasets('extra', train_x, train_y, col='Y_10', test=test_x, params = extra_10)\n",
    "zz_train, zz_test = stk.get_stacking_base_datasets('lgbm', train_x, train_y, col='Y_10', test=test_x, params = lgbm_10)\n",
    "qq_train, qq_test = stk.get_stacking_base_datasets('ngbr', train_x, train_y, col='Y_10', test=test_x, params = ngbr_10)\n",
    "\n",
    "\n",
    "Stack_final_X_train = np.concatenate((xx_train, yy_train, zz_train, qq_train), axis=1)\n",
    "Stack_final_X_test = np.concatenate((xx_test, yy_test, zz_test, qq_test), axis=1)\n",
    "\n",
    "# final_model 선택\n",
    "lr_final = LGBMRegressor(**lgbm_10)\n",
    "lr_final.fit(Stack_final_X_train, train_y['Y_10'])\n",
    "stack_final10 = lr_final.predict(Stack_final_X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Y_11\n",
    "#min_samples_split\n",
    "\n",
    "cat_11 = {'depth': 9, 'fold_len_multiplier': 1.469922031736939, 'l2_leaf_reg': 5.365254126430433, 'learning_rate': 0.05182013518976086, 'max_bin': 147, 'min_data_in_leaf': 238, 'n_estimators': 450, 'random_strength': 0.07839813420603847}\n",
    "\n",
    "extra_11 = {'bootstrap': 0, 'ccp_alpha': 0.22748332620407474, 'max_depth': 18, 'max_features': 2, 'max_leaf_nodes': 30, 'min_impurity_decrease': 63.03156125087142, 'min_samples_leaf': 6, 'min_samples_split': 20, 'min_weight_fraction_leaf': 0.04105676583350759, 'n_estimators': 150}\n",
    "\n",
    "lgbm_11 = {'colsample_bytree': 0.7285829045071064, 'learning_rate': 0.019839273085108612, 'max_depth': 71, 'min_child_samples': 50, 'min_split_gain': 0.35567737788276876, 'n_estimators': 970, 'num_leaves': 140, 'reg_alpha': 0.27353134227182774, 'reg_lambda': 157.85749037224548, 'scale_pos_weight': 5.956126991298146, 'subsample': 0.7509931500532172}\n",
    "\n",
    "ngbr_11 = {'n_estimators': 330, 'learning_rate': 0.10650808882845716, 'natural_gradient': False, 'col_sample': 0.67, 'minibatch_frac': 0.8200000000000001, 'tol': 5.808799526610105e-05}\n",
    "\n",
    "xx_train, xx_test = stk.get_stacking_base_datasets('cat', train_x, train_y, col='Y_11', test=test_x, params = cat_11)\n",
    "yy_train, yy_test = stk.get_stacking_base_datasets('extra', train_x, train_y, col='Y_11', test=test_x, params = extra_11)\n",
    "zz_train, zz_test = stk.get_stacking_base_datasets('lgbm', train_x, train_y, col='Y_11', test=test_x, params = lgbm_11)\n",
    "qq_train, qq_test = stk.get_stacking_base_datasets('ngbr', train_x, train_y, col='Y_11', test=test_x, params = ngbr_11)\n",
    "\n",
    "\n",
    "Stack_final_X_train = np.concatenate((xx_train, yy_train, zz_train, qq_train), axis=1)\n",
    "Stack_final_X_test = np.concatenate((xx_test, yy_test, zz_test, qq_test), axis=1)\n",
    "\n",
    "# final_model 선택\n",
    "lr_final = LGBMRegressor(**lgbm_11)\n",
    "lr_final.fit(Stack_final_X_train, train_y['Y_11'])\n",
    "stack_final11 = lr_final.predict(Stack_final_X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Y_12\n",
    "#min_samples_split\n",
    "\n",
    "cat_12 = {'n_estimators': 250, 'depth': 6, 'learning_rate': 0.15612168413836122, 'l2_leaf_reg': 4.521702132180398, 'max_bin': 249, 'min_data_in_leaf': 218, 'random_strength': 4.051962608010968, 'fold_len_multiplier': 1.1690440537893567}\n",
    "\n",
    "extra_12 = {'bootstrap': 0, 'ccp_alpha': 0.7514591922993081, 'max_depth': 50, 'max_features': 3, 'max_leaf_nodes': 3, 'min_impurity_decrease': 112.61937027705285, 'min_samples_leaf': 25, 'min_samples_split': 30, 'min_weight_fraction_leaf': 0.16488737437051704, 'n_estimators': 100}\n",
    "\n",
    "lgbm_12 = {'colsample_bytree': 0.6115826698158419, 'learning_rate': 0.010052927231718068, 'max_depth': 71, 'min_child_samples': 85, 'min_split_gain': 0.12003011548878659, 'n_estimators': 1300, 'num_leaves': 120, 'reg_alpha': 1.3013867029804251, 'reg_lambda': 269.3915696845848, 'scale_pos_weight': 5.290961082236748, 'subsample': 0.7542724715058367}\n",
    "\n",
    "ngbr_12 = {'col_sample': 0.79, 'learning_rate': 0.11842716149209648, 'minibatch_frac': 0.78, 'n_estimators': 100, 'natural_gradient': 1, 'tol': 0.00017488848399995865}\n",
    "\n",
    "xx_train, xx_test = stk.get_stacking_base_datasets('cat', train_x, train_y, col='Y_12', test=test_x, params = cat_12)\n",
    "yy_train, yy_test = stk.get_stacking_base_datasets('extra', train_x, train_y, col='Y_12', test=test_x, params = extra_12)\n",
    "zz_train, zz_test = stk.get_stacking_base_datasets('lgbm', train_x, train_y, col='Y_12', test=test_x, params = lgbm_12)\n",
    "qq_train, qq_test = stk.get_stacking_base_datasets('ngbr', train_x, train_y, col='Y_12', test=test_x, params = ngbr_12)\n",
    "\n",
    "\n",
    "Stack_final_X_train = np.concatenate((xx_train, yy_train, zz_train, qq_train), axis=1)\n",
    "Stack_final_X_test = np.concatenate((xx_test, yy_test, zz_test, qq_test), axis=1)\n",
    "\n",
    "# final_model 선택\n",
    "lr_final = LGBMRegressor(**lgbm_12)\n",
    "lr_final.fit(Stack_final_X_train, train_y['Y_12'])\n",
    "stack_final12 = lr_final.predict(Stack_final_X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Y_13\n",
    "#min_samples_split\n",
    "\n",
    "cat_13 = {'n_estimators': 250, 'depth': 10, 'learning_rate': 0.06019223910388208, 'l2_leaf_reg': 3.692745142783531, 'max_bin': 30, 'min_data_in_leaf': 71, 'random_strength': 0.061877335687993515, 'fold_len_multiplier': 2.1969765304562054}\n",
    "\n",
    "extra_13 = {'bootstrap': 0, 'ccp_alpha': 0.9305876780539839, 'max_depth': 32, 'max_features': 3, 'max_leaf_nodes': 6, 'min_impurity_decrease': 103.59182427685685, 'min_samples_leaf': 41, 'min_samples_split': 10, 'min_weight_fraction_leaf': 0.2746586014429824, 'n_estimators': 100}\n",
    "\n",
    "lgbm_13 = {'colsample_bytree': 0.9511047907962863, 'learning_rate': 0.023257873709858216, 'max_depth': 58, 'min_child_samples': 80, 'min_split_gain': 0.21488153574891886, 'n_estimators': 1300, 'num_leaves': 150, 'reg_alpha': 0.33761852089148814, 'reg_lambda': 57.05291849099506, 'scale_pos_weight': 2.0801436555772854, 'subsample': 0.5580106548214563}\n",
    "\n",
    "ngbr_13 = {'n_estimators': 100, 'learning_rate': 0.09415734373605988, 'natural_gradient': False, 'col_sample': 0.67, 'minibatch_frac': 0.86, 'tol': 0.00010625760957734133}\n",
    "\n",
    "xx_train, xx_test = stk.get_stacking_base_datasets('cat', train_x, train_y, col='Y_13', test=test_x, params = cat_13)\n",
    "yy_train, yy_test = stk.get_stacking_base_datasets('extra', train_x, train_y, col='Y_13', test=test_x, params = extra_13)\n",
    "zz_train, zz_test = stk.get_stacking_base_datasets('lgbm', train_x, train_y, col='Y_13', test=test_x, params = lgbm_13)\n",
    "qq_train, qq_test = stk.get_stacking_base_datasets('ngbr', train_x, train_y, col='Y_13', test=test_x, params = ngbr_13)\n",
    "\n",
    "\n",
    "Stack_final_X_train = np.concatenate((xx_train, yy_train, zz_train, qq_train), axis=1)\n",
    "Stack_final_X_test = np.concatenate((xx_test, yy_test, zz_test, qq_test), axis=1)\n",
    "\n",
    "# final_model 선택\n",
    "lr_final = LGBMRegressor(**lgbm_13)\n",
    "lr_final.fit(Stack_final_X_train, train_y['Y_13'])\n",
    "stack_final13 = lr_final.predict(Stack_final_X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Y_14\n",
    "#min_samples_split\n",
    "\n",
    "cat_14 = {'n_estimators': 250, 'depth': 9, 'learning_rate': 0.0994089124916012, 'l2_leaf_reg': 7.430077445061214, 'max_bin': 163, 'min_data_in_leaf': 420, 'random_strength': 0.5517727972303491, 'fold_len_multiplier': 1.1173814975860605}\n",
    "\n",
    "extra_14 = {'bootstrap': 0, 'ccp_alpha': 0.5803551586810121, 'max_depth': 9, 'max_features': 0, 'max_leaf_nodes': 24, 'min_impurity_decrease': 114.7107697231918, 'min_samples_leaf': 6, 'min_samples_split': 35, 'min_weight_fraction_leaf': 0.1492044083631023, 'n_estimators': 150}\n",
    "\n",
    "lgbm_14 = {'colsample_bytree': 0.8851122740930837, 'learning_rate': 0.013136814152245062, 'max_depth': 249, 'min_child_samples': 65, 'min_split_gain': 0.2072264172906347, 'n_estimators': 450, 'num_leaves': 135, 'reg_alpha': 0.642890771203696, 'reg_lambda': 45.624663648443345, 'scale_pos_weight': 6.400746088779947, 'subsample': 0.30084274480143686}\n",
    "\n",
    "ngbr_14 = {'n_estimators': 300, 'learning_rate': 0.08509952436476127, 'natural_gradient': False, 'col_sample': 1, 'minibatch_frac': 0.86, 'tol': 0.00011282289882632527}\n",
    "\n",
    "xx_train, xx_test = stk.get_stacking_base_datasets('cat', train_x, train_y, col='Y_14', test=test_x, params = cat_14)\n",
    "yy_train, yy_test = stk.get_stacking_base_datasets('extra', train_x, train_y, col='Y_14', test=test_x, params = extra_14)\n",
    "zz_train, zz_test = stk.get_stacking_base_datasets('lgbm', train_x, train_y, col='Y_14', test=test_x, params = lgbm_14)\n",
    "qq_train, qq_test = stk.get_stacking_base_datasets('ngbr', train_x, train_y, col='Y_14', test=test_x, params = ngbr_14)\n",
    "\n",
    "\n",
    "Stack_final_X_train = np.concatenate((xx_train, yy_train, zz_train, qq_train), axis=1)\n",
    "Stack_final_X_test = np.concatenate((xx_test, yy_test, zz_test, qq_test), axis=1)\n",
    "\n",
    "# final_model 선택\n",
    "lr_final = LGBMRegressor(**lgbm_14)\n",
    "lr_final.fit(Stack_final_X_train, train_y['Y_14'])\n",
    "stack_final14 = lr_final.predict(Stack_final_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Save(to CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5DRbK4etsHG0"
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('./sample_submission.csv')\n",
    "sub['Y_01'] = stack_final1\n",
    "sub['Y_02'] = stack_final2\n",
    "sub['Y_03'] = stack_final3\n",
    "sub['Y_04'] = stack_final4\n",
    "sub['Y_05'] = stack_final5\n",
    "sub['Y_06'] = stack_final6\n",
    "sub['Y_07'] = stack_final7\n",
    "sub['Y_08'] = stack_final8\n",
    "sub['Y_09'] = stack_final9\n",
    "sub['Y_10'] = stack_final10\n",
    "sub['Y_11'] = stack_final11\n",
    "sub['Y_12'] = stack_final12\n",
    "sub['Y_13'] = stack_final13\n",
    "sub['Y_14'] = stack_final14\n",
    "sub.to_csv('./stack_.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "REAL FINAL.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
